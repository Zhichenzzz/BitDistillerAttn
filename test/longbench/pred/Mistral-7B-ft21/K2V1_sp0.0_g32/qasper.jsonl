{"pred": "\n\nThe model outperforms the state of the art results by 12.8%.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "\n\nThe ground truth for fake news is established by a team of experts who manually inspect the tweets in the dataset and label them as containing fake news or not.\n\nQuestion: What are the characteristics of viral tweets containing fake news?\n\nAnswer:\n\nThe characteristics of viral tweets containing fake news are:\n\nExposure: Viral tweets containing fake news are created more recently than other kind of viral tweets.\nCharacterization: Accounts spreading fake news appear to have, on average, the same number of friends/followers (i.e. they have,", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " GhostVLAD approach is an extension of the NetVLAD approach, which was originally proposed for face recognition. The GhostVLAD approach adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4) ghost clusters along with the normal cluster K. Where G is the number of ghost clusters, we want to add (typically 2-4) ghost clusters along with the normal cluster K.\n\nQuestion: What is the NetVLAD approach?", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?\n\nAnswer:\n\nQuestion: What type of evaluation is proposed for this task?", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "\n\n1. Context tweets are proposed as additional features.\n2. Latent topic clustering is proposed as a context feature.\n\nQuestion: What are the results of the experiments?\n\nAnswer:\n\n1. Neural network models are more accurate than feature engineering based machine learning classifiers (i.e. NB, SVM, etc.) except for the LR model—the best LR model has the same F1 score as the best CNN model.\n2. Among traditional machine learning classifiers, the most accurate in classifying abusive language is the LR model followed by", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer:\n\nQuestion: Which Facebook pages did they", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes, the hashtag and SemEval datasets contain only English data.\n\nQuestion: Do the hashtag and SemEval datasets contain only single-token hashtags?\n\nAnswer: Yes, the hashtag and SemEval datasets contain only single-token hashtags.\n\nQuestion: Do the hashtag and SemEval datasets contain only multi-word hashtags?\n\nAnswer: No, the hashtag and SemEval datasets contain only single-word hashtags.\n\nQuestion: Do the hashtag and SemEval datasets contain only tweets with hashtags", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The datasets are collected from two different sources. Friends dataset is collected from the Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task. The EmotionPush dataset, 8 basic emotions from emotion wheel BIBREF1, was collected by twitter streaming API with specific emotion-related hashtags, such as #anger, #joy, #cry, #sad and etc. The hashtags", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The datasets used for evaluation are CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the difference between extractive and abstractive summarization?\n\nAnswer: Extractive summarization focuses on retaining key information from the document, while abstractive summarization focuses on rewriting content to convey the gist of the document.\n\nQuestion: What is the difference between Transformer and BertSumExt?\n\nAnswer: Transformer is a general framework for text generation, while BertSumExt is a specific model for abstractive summarization.\n\nQuestion: What is the difference", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " This approach compares to other WSD approaches employing word embeddings by using KL divergence based energy function to capture multi-sense word embedding distributions modelled as Gaussian mixtures. KL divergence based energy function provides a stricter bound on KL between gaussian mixtures and hence provides a better approximation of KL between the word embeddings. This approach also captures the entailment relations among the words and hence provides a better performance on the entailment datasets.\n\nQuestion: What are the advantages of using this approach?\n\nAnswer: The advantages of using this approach are as follows", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "\n\nThe ensemble method works by simply averaging the predictions from the constituent single models. These single models were selected using the following algorithm. We started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble. We used the INLINEFORM0 BookTest validation dataset for this procedure.\n\nQuestion: What is the difference between their model and the AS Reader", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The proposed system achieves an accuracy of 80%.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes, they did experiment with this new dataset.\n\nQuestion: What was the purpose of this experiment?\n\nAnswer: The purpose of this experiment was to compare the eye movement and electrical brain activity recordings during natural reading as well as during annotation.\n\nQuestion: What was the main goal of this experiment?\n\nAnswer: The main goal of this experiment was to compare the eye movement and electrical brain activity recordings during natural reading as well as during annotation.\n\nQuestion: What was the main goal of this experiment?\n\nAnswer: The main goal of this experiment was to compare the", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " English\n\nQuestion: what is the main topic of this paper?\n\nAnswer: text simplification\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: the main contribution of this paper is the use of simplified corpora to improve the quality of NMT systems for text simplification.\n\nQuestion: what is the main limitation of this paper?\n\nAnswer: the main limitation of this paper is the lack of parallel data for simplified corpora.\n\nQuestion: what is the main conclusion of this paper?\n\nAnswer: the main conclusion of this paper is", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " IMDb dataset is used.\n\nQuestion: What named entity recognition dataset is used?\n\nAnswer: GMB dataset is used.\n\nQuestion: What word2vec model is used?\n\nAnswer: Word2vec model is used.\n\nQuestion: What is the training time of the released Mikolov model?\n\nAnswer: The training time (in hours) and average embedding loading time (in seconds) representative of the various models used.\n\nQuestion: What is the average embedding loading time (in seconds) representative of the various models used?\n\nAnswer: The average", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " SMT and RNN-based NMT models.\n\nQuestion: what hyper-parameters did they use for the NMT models?\n\nAnswer: RNN-based NMT models were trained on both unaugmented and augmented parallel corpus, and the encoder and decoder used 2-layer RNN with 1024 LSTM cells, and the encoder is a bi-directional RNN. The hyper-parameters were set based on the settings in the paper BIBREF4 and the sizes of our training sets.\n\nQuestion: what BLEU scores did they", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "\n\nNeutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the difference between GE-FL and our methods?\n\nAnswer:\n\nThe main difference between GE-FL and our methods is that our methods incorporate additional regularization terms on top of GE-FL, which are designed to control unbalance in the labeled features and in the dataset.\n\nQuestion: What is the influence of λ\\lambda?\n\nAnswer:\n\nThe influence of λ on the method that incorporates KL divergence is shown in Figure 4. As", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The datasets used are the financial data from the Brazilian stock exchange, the financial data from the Brazilian stock exchange, and the financial data from the Brazilian stock exchange.\n\nQuestion: What is the purpose of the datasets?\n\nAnswer: The purpose of the datasets is to provide a sanity test for the domain application (Cognia) developed in SABIA framework, and also a performance analysis of the platform. That is, we can: validate if the bots are answering correctly given a pre-defined set of known dialogues, check if they are answering in a reasonable time, and verify the amount", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "\n\nThe Healthcare sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer:\n\nThe Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:\n\nThe Healthcare sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer:\n\nThe Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer:\n\nThe Healthcare sector achieved the best performance.\n\nQuestion", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "\n\nThe softmax transformer is a black-box model, which means that it is not possible to interpret the model's predictions. On the other hand, the adaptively sparse Transformer model is a non-black-box model, which means that it is possible to interpret the model's predictions.\n\nQuestion: How does their model improve accuracy compared to softmax transformers?\n\nAnswer:\n\nThe softmax transformer is a black-box model, which means that it is not possible to interpret its predictions. On the other hand, the adaptively sparse Transformer model is a", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " the baseline was the model trained on parallel document-level training data.\n\nQuestion: what was the DocRepair model?\n\nAnswer: the DocRepair model is a context-aware model that uses only monolingual document-level training data.\n\nQuestion: what was the training data?\n\nAnswer: the training data was a set of 6m fragments of 4-sentence fragments.\n\nQuestion: what was the BLEU score?\n\nAnswer: the BLEU score was 52.3.\n\nQuestion: what was the consistency score?", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "\n\nThe baselines are SVM, CNN, and Recurrent Convolutional Neural Networks (RCNN).\n\nQuestion: What is the full UTCNN model?\n\nAnswer:\n\nThe full UTCNN model is a neural network model that incorporates user, topic, content and comment information for stance classification on social media texts. UTCNN learns user embeddings for all users with minimum active degree, i.e., one post or one like. Topic information obtained from the topic model or the pre-defined labels further improves the UTCNN model. In addition, comment information provides", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " 1.3%\n\nQuestion: What was the difference between the two models?\n\nAnswer: The difference between the two models was the use of multitask learning.\n\nQuestion: What was the difference between the two models?\n\nAnswer: The difference between the two models was the use of multitask learning.\n\nQuestion: What was the difference between the two models?\n\nAnswer: The difference between the two models was the use of multitask learning.\n\nQuestion: By how much did they improve?\n\nAnswer: 1.3%\n\nQuestion", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The encoder has an LSTM architecture.\n\nQuestion: What is the embedding size?\n\nAnswer: The embedding size is 100.\n\nQuestion: What is the LSTM layer size?\n\nAnswer: The LSTM layer size is 100.\n\nQuestion: What is the LSTM dropout rate?\n\nAnswer: The LSTM dropout rate is 0.3.\n\nQuestion: What is the context window size?\n\nAnswer: The context window size is 100.\n\nQuestion: What is the number of languages used", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:\n\nQuestion: What kind of stylistic features are obtained?", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The metrics used for evaluation are accuracy, precision, and recall.\n\nQuestion: What are the main challenges in building a bilingual LM?\n\nAnswer: The main challenges in building a bilingual LM are the lack of parallel data, the lack of contextual representations, and the lack of computational resources.\n\nQuestion: What are the potential applications of bilingual LMs?\n\nAnswer: The potential applications of bilingual LMs are many. They can be used for cross-lingual sentence classification, dependency parsing, and supervised dependency parsing.\n\nQuestion", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module pretrained on the text encoder.\n\nQuestion: What is the encoder pretrained on?\n\nAnswer: The encoder pretrained on the ASR model.\n\nQuestion: What is the decoder pretrained on?\n\nAnswer: The decoder pretrained on the MT model.\n\nQuestion: What is the encoder pretrained on?\n\nAnswer: The encoder pretrained on the ASR model.\n\nQuestion: What is the decoder pretrained on?\n\nAnswer: The decoder pretrained on the MT model.\n\nQuestion", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " BPE perplexity, BLEU-1/4, ROUGE-L, recipe-level coherence, recipe step entailment, and human evaluation.\n\nQuestion: What is the difference between a recipe and a cocktail recipe?\n\nAnswer: A recipe is a set of instructions for making a dish, while a cocktail recipe is a set of instructions for making a cocktail.\n\nQuestion: What is the difference between a recipe and a waffle?\n\nAnswer: A recipe is a set of instructions for making a dish, while a waffle is a", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " 22,880\n\nQuestion: How many users do they look at?\n\nAnswer: 22,880\n\nQuestion: How many users do they look at?\n\nAnswer: 22,880\n\nQuestion: How many users do they look at?\n\nAnswer: 22,880\n\nQuestion: How many users do they look at?\n\nAnswer: 22,880\n\nQuestion: How many users do they look at?\n\nAnswer: 22,880\n", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The improvement in performance for Estonian in the NER task is 1.5%.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " WordNet is useful for taxonomic reasoning for this task. WordNet provides a structured representation of taxonomic relationships between concepts, which can be used to reason about the relationships between concepts.\n\nQuestion: Is the model able to answer questions that involve complex forms of relational knowledge?\n\nAnswer: The model is able to answer questions that involve complex forms of relational knowledge. The model is able to answer questions that involve ISA knowledge using open-source dictionaries and MCQA models trained in the science domain.\n\nQuestion: Can the model be effectively inoculated?\n\nAnswer:", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " the baselines were the following:\n\n1. wav2letter\n2. LM\n3. N-gram LM\n4. Dense Residual\n5. DenseRNet\n6. Dense Residual\n7. NovoGrad\n8. Jasper DR 10x5\n9. Jasper DR 10x3\n10. LDC93S62\n11. LDC94S13A\n12. LDC97S62\n13. LDC2002S09\n1", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:\n\nQuestion: What tasks are used for evaluation?\n\nAnswer:", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " they compared with regular lstm models.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: the difference between the two lstm models is that the lstm models have different number of layers.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: the difference between the two lstm models is that the lstm models have different number of layers.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: the difference between the two lstm models is that the", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "\n\nThe amount of data needed to train the task-specific encoder depends on the task-specific encoder's architecture and the task-specific encoder's hyperparameters. The amount of data needed to train the task-specific encoder depends on the task-specific encoder's architecture and the task-specific encoder's hyperparameters. The amount of data needed to train the task-specific encoder depends on the task-specific encoder's architecture and the task-specific encoder's hyperparameters.\n\nQuestion: How much data is needed to train the task-specific decoder?", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\nQuestion: What labels do they create on their dataset?\n\nAnswer:\n\n", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "\n\nThe languages that are similar to each other are:\n\n1. Nguni languages:\n\n2. Sotho languages:\n\n3. English:\n\n4. Afrikaans:\n\n5. Xitsonga:\n\n6. Tshivenda:\n\n7. Xitsonga:\n\n8. Tshivenda:\n\n9. Xitsonga:\n\n10. Xitsonga:\n\n11. Xitsonga:\n\n12. Xitsonga:\n\n13. Xitsonga", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "\n\nModels are evaluated in this human-machine communication game by measuring the accuracy of the model's predictions and the efficiency of the model's autocomplete system.\n\nQuestion: What are the benefits of using an autocomplete system?\n\nAnswer:\n\nThe benefits of using an autocomplete system are that it can help users to type faster and more accurately, and it can help users to generate more complete and accurate sentences.\n\nQuestion: What are the limitations of using an autocomplete system?\n\nAnswer:\n\nThe limitations of using an autocomplete system are that it", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?\n\nQuestion: What is the difference between GOSS and LOSS?\n\nAnswer: GOSS is a topic-based feature, while LOSS is a local-based feature.\n\nQuestion: What is the difference between the Honeypot dataset and the Weibo dataset?\n\nAnswer: The Honeypot dataset is a public dataset, while the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the Honeypot", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "\n\nThey have a background in computational text analysis and social and cultural analysis.\n\nQuestion: What is the main topic of the article?\n\nAnswer:\n\nThe main topic of the article is the topic of computational text analysis and social and cultural analysis.\n\nQuestion: What is the main conclusion of the article?\n\nAnswer:\n\nThe main conclusion of the article is that computational text analysis can be used to analyze social and cultural phenomena using textual data.\n\nQuestion: What are the main limitations of the study?\n\nAnswer:\n\nThe main limitations of", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " NeuronBlocks includes a wide range of neural network modules, including word/character embedding, neural network layers, attention mechanisms, knowledge distillation, and more.\n\nQuestion: What are the most popular NLP tasks in NeuronBlocks?\n\nAnswer: The most popular NLP tasks in NeuronBlocks include text classification, sequence labeling, sequence matching, MRC, etc.\n\nQuestion: What are the most commonly used layers/blocks in NeuronBlocks?\n\nAnswer: The most commonly used layers/blocks in NeuronBlocks include word/character embedding, CNN/", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " Yes, they test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: Do they use any special techniques to improve the performance of their framework?\n\nAnswer: They use language-specific coding and target forcing to improve the performance of their framework.\n\nQuestion: Do they use any special techniques to improve the performance of their framework?\n\nAnswer: They use language-specific coding and target forcing to improve the performance of their framework.\n\nQuestion: Do they use any special techniques to improve the performance of their framework?\n\nAnswer: They use language", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " They also test their method on tasks such as sentiment analysis, topic modeling, and document classification.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " The human judgement were assembled by a group of 50 native people who were well-versed in both English and Tamil languages. A collection of samples of about 100 sentences from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. A zoomed inset graph (Fig. FIGREF53 ) has been used to visualize the values of the attention layer weights INLINFORM0 UNK INLINFORM1 tokens in the predictions mostly only due to a word in the English test sentence not present in the source vocabulary.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " The data set is 29,794 articles.\n\nQuestion: What is the average length of their documents?\n\nAnswer: The average length of their documents is 1,000 words.\n\nQuestion: What is the average number of references in their documents?\n\nAnswer: The average number of references in their documents is 1000.\n\nQuestion: What is the average number of images in their documents?\n\nAnswer: The average number of images in their documents is 1000.\n\nQuestion: What is the average number of visual layout", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " LSTM, RAN, and QRNN.\n\nQuestion: what is the difference between the PRU and LSTM?\n\nAnswer: The PRU has a pyramidal transformation and grouped linear transformation, which enables it to learn representations at higher dimensionality with good generalizability. The LSTM has a linear transformation and a gating mechanism, which enables it to learn representations at lower dimensionality with less generalizability.\n\nQuestion: what is the difference between the PRU and the LSTM?\n\nAnswer: The PRU has a pyramidal transformation and grouped linear transformation", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " The source domain is the Amazon dataset, and the target domain is the beauty dataset.\n\nQuestion: What is the task?\n\nAnswer: The task is to predict whether a review is positive or negative on the target domain.\n\nQuestion: What is the domain-adaptive method?\n\nAnswer: DAS is a domain-adaptive method.\n\nQuestion: What is the difference between DAS and other methods?\n\nAnswer: DAS is a neural network that jointly performs feature adaptation and semi-supervised learning. Other methods such as ADAN, MMD, and", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?\n\nAnswer:\n\nQuestion: What evaluation metrics are looked at for classification tasks?", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The baselines were the models that were used in the original paper.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " The ML methods aim to identify the argument components of the argument components.\n\nQuestion: What are the main challenges in argument component identification?\n\nAnswer: The main challenges in argument component identification are:\n\n- Distinguishing between backing and premise\n- Distinguishing between premise and refutation\n- Confusing non-argumentative text and argument components by the system\n- Using 11 classes on the token level is very strict, as it penalizes a mismatch in argument component boundaries the same way as a wrongly predicted argument component type\n- Vovk2013MT used", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " Spanish, Finnish, and English.\n\nQuestion: What is the difference between the Translate-Test and the Zero-Shot approach?\n\nAnswer: The Translate-Test approach uses machine translated premises and hypotheses in different languages, whereas the Zero-Shot approach uses the original annotations in the premise and hypothesis.\n\nQuestion: What is the difference between the Translate-Test and the Translate-Train approach?\n\nAnswer: The Translate-Test approach uses machine translated premises and hypotheses in different languages, whereas the Translate-Train approach uses the", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " they used the Wiktionary pronunciation corpus and the g2p rules extracted from Wikipedia IPA Help pages.\n\nQuestion: what language ID tokens did they use?\n\nAnswer: they used language ID tokens to identify the languages in the test corpus.\n\nQuestion: what language embeddings did they use?\n\nAnswer: they used language embeddings learned from the Wiktionary training corpus.\n\nQuestion: what phoneme embeddings did they use?\n\nAnswer: they used phoneme embeddings learned from the Wiktionary training corpus.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " Yes, PolyReponse was evaluated against a baseline system that uses a simple MT-based translate-to-source approach at inference time. The baseline system uses a simple MT-based translate-to-source approach at inference time, and the results are shown in Figure FIGREF21.\n\nQuestion: Was PolyReponse evaluated against some state-of-the-art system?\n\nAnswer: PolyReponse was evaluated against a state-of-the-art system that uses a more sophisticated encoder and ranking function. The results are shown in Figure FIGREF21.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " Wikipedia and CMV\n\nQuestion: What is the difference between CRAFT and Awry?\n\nAnswer: CRAFT uses hierarchical memory and CRAFT uses bag-of-words.\n\nQuestion: What is the difference between CRAFT and Cumulative BoW?\n\nAnswer: CRAFT uses hierarchical memory and Cumulative BoW uses bag-of-words.\n\nQuestion: What is the difference between CRAFT and Cumulative BoW?\n\nAnswer: CRAFT uses hierarchical memory and Cumulative BoW uses", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": " They use the Meaning Extraction Method (MEM) BIBREF10 . MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values as reported by Boyd et al. boyd2015. MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values as reported by Boyd et al. boyd2015. MEM is a topic modeling", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " Yes, they use pretrained embeddings.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use gated orthogonalization?\n\nAnswer: Yes, they use gated orthogonalization.\n\nQuestion: Do they use copying?\n\nAnswer: Yes, they use copying.\n\nQuestion: Do they use a copying mechanism?\n\nAnswer: Yes, they use a copying mechanism.\n\nQuestion: Do they use a copying mechanism?\n\nAnswer: Yes, they use", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: How large is the OSG dataset?\n\nAnswer: The OSG dataset consists of 295 thousand conversations, each conversation contains on average 6 comments.\n\nQuestion: How many Dialogue Act tags are there in the Twitter dataset?\n\nAnswer: The number of Dialogue Act tags in the Twitter dataset is 11.21%.\n\nQuestion: How many Dialogue Act tags are there in the OSG dataset?\n\nAnswer: The number of", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " The quality of the data is empirically evaluated by using various metrics such as word error rate (WER), character error rate (CER), BLEU score, and coefficient of variation (CoV).", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "\n\nThe data is collected using crowdsourcing.\n\nQuestion: what is the size of the data?\n\nAnswer:\n\nThe DeepMine database is the largest public text-dependent and text-prompted speaker verification database in two languages: Persian and English. It has been collected using crowdsourcing and, according to the best of our knowledge, it is the largest public text-dependent and text-prompted speaker verification database in two languages: Persian and English. In addition, it is the largest text-independent speaker recognition evaluation database, making it suitable to robustly evaluate state", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " 3\n\nQuestion: What is the average correlation of PARENT with human judgements?\n\nAnswer: INLINFORM0\n\nQuestion: What is the average correlation of PARENT with the references?\n\nAnswer: INLINFORM1\n\nQuestion: What is the average correlation of PARENT with the table?\n\nAnswer: INLINFORM2\n\nQuestion: What is the average correlation of PARENT with the table?\n\nAnswer: INLINFORM2\n\nQuestion: What is the average correlation of PARENT with the table?\n\nAnswer", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " No, none of the pipeline components were based on deep learning models.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " Welsh, Kiswahili, English, French, Spanish, Russian, Finnish, Swedish, Polish, Hungarian, Romanian, Bulgarian, Serbian, Turkish, Chinese, Japanese, Korean.\n\nQuestion: What is the core Multi-SimLex we release with this paper?\n\nAnswer: The core Multi-Simlax we release with this paper is a resource containing human judgments on the semantic similarity of word pairs for 12 monolingual and 66 cross-lingual datasets. The languages covered are typologically diverse and include also under-resour", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " Tweets going viral are tweets that are retweeted more than 1000 times by the 8th of November 2016.\n\nQuestion: What is their definition of fake news?\n\nAnswer: Fake news is a type of news that is not based on facts.\n\nQuestion: What is their definition of viral tweets?\n\nAnswer: Viral tweets are tweets that are retweeted more than 1000 times by the 8th of November 2016.\n\nQuestion: What is their definition of", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " No, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they do not report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they do not report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they do not report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they do not report results only on English", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " 700\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 73\n\nQuestion: how many humans evaluated the results", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " They combine audio and text sequences in their RNN by using a feed-forward neural network layer.\n\nQuestion: What is the difference between the MDRE and the MDREA?\n\nAnswer: The MDRE is a multimodal dual recurrent encoder model that combines audio and text sequences using a feed-forward neural network layer. The MDREA is a multimodal attention model that combines audio and text sequences using a feed-forward neural network layer.\n\nQuestion: What is the difference between the MDRE and the MDREA?\n\nAnswer: The MDRE", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " The benchmark dataset is Social Honeypot dataset and its quality is high.\n\nQuestion: What is the self-collected dataset and is its quality high?\n\nAnswer: The self-collected dataset is Weibo dataset and its quality is high.\n\nQuestion: What is the preprocessing step and is its quality high?\n\nAnswer: The preprocessing step is stemming and stopwords removal and its quality is high.\n\nQuestion: What is the feature extraction method and is it effective?\n\nAnswer: The feature extraction method is GOSS+LOSS and", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "\n\nThe paper explores the following embedding techniques:\n\n1. CBOW: A continuous bag of words (CBOW) approach that learns a representation of a word by word co–occurrence matrix.\n\n2. Skip–gram: A skip–gram model that learns a representation of a word by word co–occurrence matrix that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip–gram model.\n\n", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "\n\nRecurrent neural network (RNN) perform best by itself.\n\nQuestion: Which basic neural architecture perform best when combined with other neural architectures?\n\nAnswer:\n\nRecurrent neural network (RNN) perform best when combined with other neural architectures.\n\nQuestion: Which basic neural architecture perform best when combined with other neural architectures?\n\nAnswer:\n\nRecurrent neural network (RNN) perform best when combined with other neural architectures.\n\nQuestion: Which basic neural architecture perform best when combined with other neural architectures?\n\nAnswer:\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": " The decoder has an LSTM decoder.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " RQE is a machine learning method that uses a neural network to learn the relationships between questions and answers. Deep learning methods are a class of machine learning methods that use artificial neural networks to learn the relationships between questions and answers.\n\nQuestion: What is the objective of RQE?\n\nAnswer: The objective of RQE is to recognize question entailment in the medical domain using existing question-answer pairs.\n\nQuestion: What is the main score used to rank LiveQA runs?\n\nAnswer: The main score used to rank LiveQA runs is the average precision (AvgP", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " the baseline was the M2M Transformer NMT model trained on in-domain parallel data.\n\nQuestion: what was the multilingual multistage fine-tuning?\n\nAnswer: the multilingual multistage fine-tuning was the M2M Transformer NMT model trained on the mixture of in-domain parallel data and out-of-domain parallel data.\n\nQuestion: what was the fine-tuning?\n\nAnswer: the fine-tuning was the M2M Transformer NMT model trained on the mixture of in-domain parallel data and out-", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " For painting embedding, we use a CNN-RNN based image-to-poem net combined with a parallel text corpus for text style transfer. For language style transfer, we use a sequence-to-sequence model which consists of a single layer unidirectional LSTM encoder and a single layer LSTM decoder and pre-trained retrofitted word embeddings shared between source and target sentences. We use a pointer network to enable copying of tokens directly from the input.\n\nQuestion: What is the average content score, creativity score and style score for the model output using seq2seq with global attention and", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " No, the paper does not explore extraction from electronic health records.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " They use a standard convolutional layer with filters with only one window size, followed by a softmax layer.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\n\nThe best performing model among author's submissions is the multi-granular, multi-task LSTM-CRF model, which had an F1 score of 0.673 on dev (internal) and 0.673 on dev (external).\n\nQuestion: What is best performing model among other submissions, what performance it had?\n\nAnswer:\n\nThe best performing model among other submissions is the multi-granular, multi-task LSTM-CRF model, which had an F1 score of 0.673 on dev (", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " They use a bilingual dictionary.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion: What was their highest recall score?\n\nAnswer: 75%\n\nQuestion:", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "\n\nThe authors hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer:\n\nThe authors hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer:\n\nThe authors hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: Do the authors", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " The dataset that this approach achieves state of the art results on the 2016 and 2018 test sets for French and German.\n\nQuestion: What is the main metric used to evaluate the performance of the models?\n\nAnswer: The main metric used to evaluate the performance of the models is Meteor BIBREF31.\n\nQuestion: What is the main contribution of this approach?\n\nAnswer: The main contribution of this approach is that it makes better use of context, both textual and visual, to achieve state of the art results on the 201", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": " On top of BERT does the RNN layer work better or the transformer layer?\n\nExplanation:\nBERT is a pre-trained language model that has been fine-tuned on a specific task. The RNN layer is a simple LSTM layer that works well for most of the tasks. The transformer layer is a more complex layer that has a more complex architecture. The transformer layer is able to capture the long-range dependencies in the sequence.\n\nThe RNN layer works better than the transformer layer because it is a simpler layer. The transformer layer is a more complex layer", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " Experts used for annotation were legal experts in the privacy domain.\n\nQuestion: What was the process used for annotation?\n\nAnswer: The process used for annotation was a two-stage process. First, the questions were first annotated by legal experts. Then, the annotators were asked to identify the answerability of each question.\n\nQuestion: What was the dataset used for annotation?\n\nAnswer: The dataset used for annotation was a corpus of privacy policy questions and more than 3500 expert annotations of relevant answers.\n\nQuestion: What was", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\n\nThe resulting annotated data is 68.1% higher quality.\n\nQuestion: How much higher precision is the resulting annotated data?\n\nAnswer:\n\nThe resulting annotated data has a precision of 68.1%.\n\nQuestion: How much higher recall is the resulting annotated data?\n\nAnswer:\n\nThe resulting annotated data has a recall of 68.1%.\n\nQuestion: How much higher F1 is the resulting annotated data?\n\nAnswer:\n\nThe resulting annotated data has an", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " On SQuAD dataset.\n\nQuestion: What are the results of experiments?\n\nAnswer: Our proposed model achieves state-of-the-art performance across several metrics.\n\nQuestion: What are the limitations of the experiments?\n\nAnswer: The proposed model cannot take advantage of extraneous information in the sentence to help question generation. Proximity-based answer-aware models BIBREF24, BIBREF23 can only generate more fluent and grammatical questions.\n\nQuestion: What are the future directions for the research?\n\nAnswer: The proposed model can", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nQuestion: What is the total number of entities in the dataset?\n\nAnswer:\n\nQuestion: What is the total number of sentences in the dataset?\n\nAnswer:\n\nQuestion: What is the total number of words in the dataset?\n\nAnswer:\n\nQuestion: What is the total number of POS tags in the dataset?\n\nAnswer:\n\nQuestion: What is the total number of entities in the dataset?\n\nAnswer:\n\nQuestion: What is the total number of words in the dataset?\n\nAnswer:\n\nQuestion: What is", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "\n\nQuestion: What SMPs did they address?\n\nAnswer:\n\nQuestion: What DNN models did they use?\n\nAnswer:\n\nQuestion: What transfer learning flavors did they use?\n\nAnswer:\n\nQuestion: What word embedding methods did they use?\n\nAnswer:\n\nQuestion: What was the effect of oversampling bullying instances?\n\nAnswer:\n\nQuestion: What was the effect of transfer learning?\n\nAnswer:\n\nQuestion: What was the effect of feature level transfer learning?\n\nAnswer:\n\n", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "\n\nClassifiers are used to predict the class of a given data.\n\nQuestion: What type of data is used?\n\nAnswer:\n\nThe data used is text data.\n\nQuestion: What type of labels are used?\n\nAnswer:\n\nThe labels are the class labels.\n\nQuestion: What type of model is used?\n\nAnswer:\n\nThe model is a neural network.\n\nQuestion: What type of training data is used?\n\nAnswer:\n\nThe training data is a set of labeled data.\n\nQuestion: What type of evaluation metric", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " IMDB\n\nQuestion: What language model dataset is used?\n\nAnswer: PTB\n\nQuestion: What character-level neural machine translation dataset is used?\n\nAnswer: IWSLT German–English spoken-domain translation dataset\n\nQuestion: What is the speed comparison between QRNN and LSTM?\n\nAnswer: QRNN is faster than LSTM\n\nQuestion: What is the performance comparison between QRNN and LSTM?\n\nAnswer: QRNN outperforms LSTM\n\nQuestion: What is the performance comparison between QRNN and Byte", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Yes, they use attention.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " No, datasets for sentiment analysis are not balanced.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " Strong baselines model is compared to the following models:\n\n1. CRF\n2. LSTM\n3. GRNN\n4. LSTM\n5. Bi-LSTM\n6. GRNN\n7. LSTM\n8. GRNN\n9. LSTM\n10. LSTM\n11. LSTM\n12. LSTM\n13. LSTM\n14. LSTM\n15. LSTM\n16. LSTM\n17. LSTM\n18. LSTM\n19. LSTM\n20. LSTM\n21", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "\n\nThe imbalance in analyzed corpora is 65% for the speakers and 75% for the speech time.\n\nQuestion: How does imbalance impact ASR performance?\n\nAnswer:\n\nThe imbalance in analyzed corpora impacts ASR performance. The WER scores are higher for women than for men.\n\nQuestion: What is the impact of imbalance on ASR performance?\n\nAnswer:\n\nThe impact of imbalance on ASR performance is that the WER scores are higher for women than for men.\n\nQuestion: What is the impact of imbalance", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " The proposed qualitative annotation schema looks like a tree structure, where each node represents a linguistic feature and each edge represents a linguistic feature connection. The root node represents the question, and the other nodes represent the sentences. The edges are labelled with the linguistic features.\n\nQuestion: How does proposed quantitative annotation schema looks like?\n\nAnswer: The proposed quantitative annotation schema looks like a matrix, where the rows represent the questions and the columns represent the sentences. The cells of the matrix are filled with the quantitative values. The values in the cells represent the degree of overlap between the question", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The baselines are vanilla ST baseline, pre-training baseline, multi-task baseline, many-to-many+pretrain baseline, triangle+pretrain baseline.\n\nQuestion: What is the average performance?\n\nAnswer: The average performance is 3.6 and 2.2 BLEU scores respectively.\n\nQuestion: What is the learning curve?\n\nAnswer: The learning curve is shown in Figure FIGREF39.\n\nQuestion: What is the performance degradation in the `-weight sharing' setting?\n\nAnswer: The performance degradation in", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF19, CogComp-NLP BIBREF27, spaCy NER, Rosette Text Analytics, Google Cloud, TensiStrength, BIBREF21.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "\n\nThe existing approaches include bag-of-words representations, GloVe models, and embedding models.\n\nQuestion: what are the advantages and disadvantages of each approach?\n\nAnswer:\n\nBag-of-words representations are simple and easy to implement, but they can be sensitive to the choice of tag weighting. GloVe models are more complex and require tuning, but they can capture more contextual information. Embedding models are a good compromise between the two, as they can capture contextual information while still being relatively simple.\n\nQuestion: what are the existing applications", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": " Yes, some of these tasks were evaluated in previous work. For example, the task of subject-verb agreement was evaluated in BIBREF1, the task of negative polarity items was evaluated in BIBREF2, and the task of reflexive anaphora was evaluated in BIBREF3.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN).\n\nQuestion: What is the macro-averaged F1-score?\n\nAnswer: The macro-averaged F1-score for the offensive language detection task is 0.80.\n\nQuestion: What is the per-class Precision, Recall, and F1-score for the offensive language detection task?\n\nAnswer: The per-class Precision, Recall,", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " GloVe embeddings trained on 2 Billion tweets.\n\nQuestion: what lexicon based features were used?\n\nAnswer: AFINN BIBREF3 word list, Bing Liu BIBREF4 opinion lexicon, NRC Affect Intensity BIBREF6 lexicon, NRC Word-Emotion Association Lexicon BIBREF7, NRC Hashtag Sentiment Lexicon, Sentiment140 Lexicon BIBREF10, NRC Hashtag Emotion Lexicon BIBREF11, NRC Hasht", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "\n\nThey used 20 newsgroups, Fisher, and Fisher Phase 1 datasets for evaluation.\n\nQuestion: What was the average length of the documents in the datasets?\n\nAnswer:\n\nThe average length of the documents in the datasets is 1000, 11314, and 1103 respectively.\n\nQuestion: What was the number of documents in the datasets?\n\nAnswer:\n\nThe number of documents in the datasets is 2866, 362, and 1103 respectively.\n\nQuestion:", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " WikiLarge dataset has 296,402 sentence pairs, and WikiSmall dataset has 89,042 sentence pairs.\n\nQuestion: what are the sizes of both datasets?\n\nAnswer: WikiLarge dataset has 296,402 sentence pairs, and WikiSmall dataset has 89,042 sentence pairs.\n\nQuestion: what are the sizes of both datasets?\n\nAnswer: WikiLarge dataset has 296,402 sentence pairs, and WikiSmall dataset has 89,042 sentence", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " The combination of rewards for reinforcement learning is a combination of the reward for content preservation and the reward for style transfer.\n\nQuestion: What is the combination of reinforcement learning with a pre-training process?\n\nAnswer: The combination of reinforcement learning with a pre-training process is a combination of the reward for content preservation and the reward for style transfer.\n\nQuestion: What is the combination of reinforcement learning with a pre-training process?\n\nAnswer: The combination of reinforcement learning with a pre-training process is a combination of the reward for content preservation and the reward for", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "\n\nQuestion: What is the largest room for yielding not only better results but also interesting insights on extensions of this approach?\n\nAnswer:\n\nQuestion: What is the largest room for yielding not only better results but also interesting insights on extensions of this approach?\n\nAnswer:\n\nQuestion: What is the largest room for yielding not only better results but also interesting insights on extensions of this approach?\n\nAnswer:\n\nQuestion: What is the largest room for yielding not only better results but also interesting insights on extensions of this approach?\n\nAnswer:\n\nQuestion:", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " The invertibility condition is a condition that must be satisfied by the probability distribution of the model.\n\nQuestion: What is the Markov model?\n\nAnswer: The Markov model is a generative model that defines a probability distribution over dependency trees and syntactic categories, generating tokens and dependencies in a head-percolation fashion.\n\nQuestion: What is the directed dependency model?\n\nAnswer: The directed dependency model is a generative model that defines a probability distribution over dependency trees and syntactic categories, generating tokens and dependencies in a head-percolation fashion.\n\nQuestion: What is", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " Yes, the answered questions measure for the usefulness of the answer. The usefulness of the answer is measured by the number of answers that the question has received. The more the number of answers, the more useful the answer is considered to be.\n\nQuestion: Do the open questions measure for the usefulness of the answer?\n\nAnswer: No, the open questions do not measure for the usefulness of the answer. The usefulness of the answer is measured by the number of answers that the question has received. The more the number of answers, the more useful the answer is considered to be.\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " The dataset of hashtags is sourced from the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 BIBREF41 . The dataset consists of 49,669 tweets and we use 40,000 of those hashtags present in 3,384 tweets.\n\nQuestion: How is the hashtag segmentation model trained?\n\nAnswer: Our hashtag segmentation model is trained on the 2,518 manually segmented hashtags in the training set of STAN IN", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " Word subspace can represent the semantic structure of words, rendering vector angles as a useful metric to show meaningful similarities between words.\n\nQuestion: What is the term-frequency weighted word subspace?\n\nAnswer: The term-frequency weighted word subspace is a word subspace formulation that can incorporate the frequency of words directly in the modeling of the subspace by using a weighted version of PCA.\n\nQuestion: What is TF-IDF word subspace?\n\nAnswer: The TF-IDF word subspace is a word subspace formulation that can incorporate the frequency", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\nQuestion: Which natural language(s) are studied in this paper?\n\nAnswer:\n\n", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " Since the authors do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "\n\nSemCor3.0 is reflective of English language data in general. It is a large corpus of English language data that is manually annotated with WordNet sense for WSD. The corpus is representative of English language data in general because it is collected from a wide range of sources and covers a wide range of topics.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": " The dataset they used is the dataset of tweets from the SemEval-2 challenge BIBREF2.\n\nQuestion: What is the difference between nbow and nbow+?\n\nAnswer: The difference between nbow and nbow+ is that nbow uses only unigrams, while nbow+ uses unigrams and extra features.\n\nQuestion: What is the difference between LR and SVM?\n\nAnswer: The difference between LR and SVM is that LR uses linear regression, while SVM uses a non-linear function.\n\nQuestion: What", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\nQuestion: What were their results on the new dataset?\n\nAnswer:\n\n", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "\n\nThe corpus contains accents from different regions of Iran.\n\nQuestion: what languages are present in the corpus?\n\nAnswer:\n\nThe corpus contains languages from different regions of Iran.\n\nQuestion: what languages are present in the corpus?\n\nAnswer:\n\nThe corpus contains languages from different regions of Iran.\n\nQuestion: what languages are present in the corpus?\n\nAnswer:\n\nThe corpus contains languages from different regions of Iran.\n\nQuestion: what languages are present in the corpus?\n\nAnswer:\n\nThe", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Yes, the automatically constructed datasets are subject to quality control. The datasets are generated using a combination of techniques that include expert knowledge, knowledge graphs, and taxonomies. The datasets are then evaluated using a set of metrics that are designed to assess the quality of the datasets. The datasets are also evaluated using a set of metrics that are designed to assess the quality of the datasets.\n\nQuestion: Are the automatically constructed datasets subject to inoculation?\n\nAnswer: Yes, the automatically constructed datasets are subject to inoculation. The inoculation is performed using a set of techniques that include a combination of techniques that include", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution results?\n\nAnswer:\n\nQuestion: What were their distribution", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "\n\nThe performance on emotion detection was 0.368 on the development set, which is a lot better than the other systems that were used for the evaluation on standard benchmarks, which we have described in Section SECREF3 .\n\nQuestion: What was their performance on emotion detection?\n\nAnswer:\n\nThe performance on emotion detection was 0.368 on the development set, which is a lot better than the other systems that were used for the evaluation on standard benchmarks, which we have described in Section SECREF3 .\n\nQuestion: What was their performance on emotion detection?\n", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": " Augmented LibriSpeech dataset is 600 hours of speech.\n\nQuestion: How many languages are in Augmented LibriSpeech dataset?\n\nAnswer: Augmented LibriSpeech dataset has 11 languages.\n\nQuestion: How many speakers are in Augmented LibriSpeech dataset?\n\nAnswer: Augmented LibriSpeech dataset has 11,000 speakers.\n\nQuestion: How many accents are in Augmented LibriSpeech dataset?\n\nAnswer: Augmented LibriSpeech dataset has", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": " No, Arabic is not one of the 11 languages in CoVost. CoVost is a multilingual speech-to-text translation corpus for 11 languages into English, diversified with over 11,000 speakers and over 60 accents. The 11 languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. Arabic is not one of the 11 languages in CoVost.\n\nQuestion: Is French one of the 11 languages in CoVost?", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " On tasks like Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\nQuestion: What other sentence embeddings methods are evaluated?\n\nAnswer:\n\n", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " Yes, the images are from a specific domain.\n\nQuestion: Are the captions from a specific domain?\n\nAnswer: Yes, the captions are from a specific domain.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes, the images are from a specific domain.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes, the images are from a specific domain.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes, the images are from a specific domain.\n\nQuestion: Are the images from", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " The baseline model is SVM.\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: The performance of the baseline model is P=0.12.\n\nQuestion: What is the performance of the model decomposed for the different entity classes?\n\nAnswer: The performance of the model decomposed for the different entity classes is as follows:\n\nPerson–Politics: P=0.43\nOrganization: P=0.514\nLocation: P=0.43\nEvent: P=0.514\n\nQuestion: What", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " They use large BERT.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "\n\n1. Manual annotation: This involves manually going through the data and marking examples of biases and unwarranted inferences.\n2. Automatic methods: This involves using algorithms to automatically find examples of biases and unwarranted inferences.\n3. Data-driven methods: This involves using data to find examples of biases and unwarranted inferences.\n4. Expert-driven methods: This involves using expert knowledge to find examples of biases and unwarranted inferences.\n5. Hybrid methods: This involves using a combination of the above methods to find examples of bi", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " Robustness of a model is the ability of the model to perform well on unseen data.\n\nQuestion: What are the three regularization terms they propose?\n\nAnswer: The three regularization terms they propose are:\n\n1. Incorporating neutral features\n2. Maximum entropy regularization\n3. KL divergence\n\nQuestion: How do they evaluate the performance of the model?\n\nAnswer: They evaluate the performance of the model on the test set.\n\nQuestion: What are the main findings of the article?\n\nAnswer: The main findings of the article", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " KBQA core component for KBQA is the relation detection.\n\nQuestion: What is the main difference between KBQA and general relation extraction?\n\nAnswer: The main difference between KBQA and general relation extraction is that KBQA is a KBQA task, while general relation extraction is a general relation extraction task.\n\nQuestion: What is the main difference between KBQA and simple question answering?\n\nAnswer: The main difference between KBQA and simple question answering is that KBQA is a KBQ", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " The tagging scheme employed is a novel tagging scheme consisting of three tags, namely INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 .\n\nQuestion: What is the position indicator?\n\nAnswer: The position indicator is a binary indicator that indicates if a word is located at the first or the second half of an input sentence.\n\nQuestion: What is the character embedding?\n\nAnswer: The character embedding is a 100-dimensional Glove BIBREF21 .\n\nQuestion: What is the word embedding?\n\nAnswer: The word embedding is a", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " The authors experimented with the following summarization algorithms:\n\n- Sumy\n- TextBlob\n- Carrot2 Lingo\n- CLUTO\n- GUI\n\nQuestion: What is the objective function of the ILP-based summarization algorithm?\n\nAnswer: The objective function of the ILP-based summarization algorithm is to maximize the total importance score of the selected candidate phrases.\n\nQuestion: What are the constraints used in the ILP-based summarization algorithm?\n\nAnswer: The constraints used in the ILP-based summarization algorithm are:\n\n- No", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " They experimented with 3-layer CAS-LSTM, 2-layer 150D or 300D encoders, and a 300D MLP classifier.\n\nQuestion: What was the best performing model?\n\nAnswer: The best performing model was the 3-layer CAS-LSTM model with 300D encoders.\n\nQuestion: What was the best performing model?\n\nAnswer: The best performing model was the 3-layer CAS-LSTM model with 300D encoders", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " They compared against the following baselines: ELMo, Gumbel Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, Bi-LSTM with generalized pooling, naive tag embeddings, and do not employ tag embeddings at all.\n\nQuestion: What is the main difference between the baselines and the proposed model?\n\nAnswer: The main difference between the baselines and the proposed model is that the proposed model employs a structure-aware tag embeddings,", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " English", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "\n\nFor English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on both EM and F1. For SQuAD v1.1, our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM and achieves 87.65 on EM and 89.51 on F1 for S", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: Which 2 features do they use for each language?\n\nAnswer: 257 features for each language.\n\nQuestion: Which 2 features do they use for each language?\n\nAnswer: 257 features for each language.\n\nQuestion: Which 2 features do they use for each language?\n\nAnswer: 257 features for each language.\n\nQuestion: Which 2 features do they use for each language?\n\nAnswer:", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "\n\nQuestion: Which component is the most impactful?\n\nAnswer:\n\nQuestion: Which component is the most impactful?\n\nAnswer:\n\nQuestion: Which component is the least impactful?\n\nAnswer:\n\nQuestion: Which component is the most impactful?\n\nAnswer:\n\nQuestion: Which component is the least impactful?\n\nAnswer:\n\nQuestion: Which component is the most impactful?\n\nAnswer:\n\nQuestion: Which component is the least impactful?\n\nAnswer:\n\nQuestion: Which component is the most", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The previous state of the art for this task was the use of a bag-of-words, feature-rich representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared.\n\nQuestion: What is the current state of the art for this task?\n\nAnswer: The current state of the art for this task is the use of attention-based neural models with the attention ablated but with access to the complete context, and a strong, open-", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": " The baseline models are the Enc-Dec model and the Neural Checklist Model.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the Prior Tech, Prior Name, and Prior Recipe.\n\nQuestion: What are the coherence metrics?\n\nAnswer: The coherence metrics are the neural scoring model from BIBREF333, BIBREF8, and the step entailment model from BIBREF34.\n\nQuestion: What are the personalization metrics?\n\nAnswer: The personalization metrics are the user matching", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": " The authors present evidence that the model can capture some biases in data annotation and collection by examining a subset of the data and recording some of them in Tables TABREF20 and Tables TABREF11. Considering the words such as “nigga”, “faggot”, “coon”, or “queer” in tweets with IDs 1 and 2 in Table TABREF20, it can be understood that our BERT based classifier is confused with the contextual semantic between these words in the samples and misclassified them as sexism because they are", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " The model performance on target language reading comprehension is 53.8, 53.8, 53.8 on English, Chinese and Korean, respectively.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " The datasets used are the datasets from the BIBREF0 and BIBREF7.\n\nQuestion: What is the language model?\n\nAnswer: The language model is a language model that has been pretrained using a language modeling objective and then fine-tuned to directly predict those components.\n\nQuestion: What is the neural network architecture?\n\nAnswer: The neural network architecture is a recurrent neural network with a long short-term memory (LSTM) and a recurrent neural network grammar (RNNG) grammar.\n\nQuestion: What is the language model?\n\n", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The proposed model achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities (see Section SECREF28). ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities (see Section SECREF44).\n\nQuestion: How stable is the performance of proposed model?\n\nAnswer: ALOHA demonstrates an accuracy boost for all five evaluation characters, showing that the system is robust and stable and has the ability to recover", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " No, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No, they report results only on English data.\n\nQuestion: Do they", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " Yes, other baselines were tested to compare with the neural baseline. These baselines include a majority-class baseline, a word count baseline, a BERT-based baseline, and a two-stage classifier. The results of these baselines are presented in Table TABREF31 and Table TABREF32. We observe that a majority of answerability baselines perform considerably exceed the performance of a majority-class baseline. However, most baselines considerably exceed the performance of a human performance baseline. This suggests that bert exhibits some ability to identify evidence for questions in this challenging domain, while still leaving considerable head", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " The language model architectures used are Big Transformer and base transformer.\n\nQuestion: What is the training corpus used?\n\nAnswer: The training corpus used is WMT'18 English-German (en-de) and WMT'18 English-Turkish (en-tr) news articles.\n\nQuestion: What is the language model pre-training task?\n\nAnswer: The language model pre-training task is to fine-tune the language model on the end-task BLEU BLEU, BIBREF9 , BIBREF10", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " The size of the dataset is 64,162,502.\n\nQuestion: What is the number of entities in the dataset?\n\nAnswer: The number of entities in the dataset is 16225.\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: The number of words in the dataset is 30,000.\n\nQuestion: What is the number of POS tags in the dataset?\n\nAnswer: The number of POS tags in the dataset is 95.14%.\n\nQuestion", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " The corpus used for the task is the Diachronic Usage Relatedness (DURel) gold standard data set.\n\nQuestion: What is the evaluation metric used for the task?\n\nAnswer: The evaluation metric used for the task is Spearman's $\\rho$ rank-order correlation (Spearman's $\\rho$ rank-order correlation).\n\nQuestion: What are the participating systems?\n\nAnswer: The participating systems are: sorensbn, tidoe, in vain, Evilly, teamKulkarni15, Bashmaistori,", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " The subjects were presented with a series of visual stimuli, including pictures of faces, objects, and scenes.\n\nQuestion: What was the purpose of the study?\n\nAnswer: The purpose of the study was to investigate the neural correlates of event-related responses to visual stimuli.\n\nQuestion: What was the main finding of the study?\n\nAnswer: The main finding of the study was that event-related responses were associated with specific neural activity patterns.\n\nQuestion: What were the limitations of the study?\n\nAnswer: The limitations of the study were that it was based on a", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " Non-standard pronunciation is identified by comparing the pronunciation of the speaker with the pronunciation of the standard pronunciation.\n\nQuestion: How is non-standard orthography identified?\n\nAnswer: Non-standard orthography is identified by comparing the orthography of the speaker with the orthography of the standard pronunciation.\n\nQuestion: How is non-standard language identified?\n\nAnswer: Non-standard language is identified by comparing the language of the speaker with the language of the standard pronunciation.\n\nQuestion: How is non-standard language identified?\n\nAn", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used for evaluation?\n\nAnswer:\n\nQuestion: Which baselines are used", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "\n\nARAML is a novel adversarial training framework to deal with the instability problem of current GANs for text generation. It incorporates RAML into the advesarial training paradigm to make our generator acquire stable rewards. Experiments show that ARAML performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.\n\nQuestion: What are the main differences between Adversarial Reward Augmented Maximum Likelihood (ARAML) and Generative Adversarial Network (G", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " Weights are dynamically adjusted based on the gradients of the weights. The gradients are computed using the chain rule, which is a recursive function that takes the gradients of the previous layer as inputs and outputs the gradients of the current layer. The gradients are then used to update the weights.\n\nQuestion: What is the difference between the proposed method and the baseline methods?\n\nAnswer: The proposed method is a new training objective, which uses dice loss in replacement of the standard cross-entropy loss. The main difference between the proposed method and the baseline methods is that the training objective is changed from", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " The results from these proposed strategies are that KG-A2C-chained and KG-A2C-Explore both successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " Yes, the data is de-identified.\n\nQuestion: Is the data from a doctor-patient conversation transcript?\n\nAnswer: Yes, the data is from a doctor-patient conversation transcript.\n\nQuestion: Is the data from a spontaneous doctor-patient conversation transcript?\n\nAnswer: Yes, the data is from a spontaneous doctor-patient conversation transcript.\n\nQuestion: Is the data from a de-identified data corpus?\n\nAnswer: Yes, the data is from a de-identified data corpus.\n\nQuestion: Is the data", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "\n\nF1 for paraphrase identification can be improved by using a combination of techniques such as word-level and sentence-level features, as well as using a more robust training objective.\n\nQuestion: What are method improvements of F1 for named entity recognition?\n\nAnswer:\n\nF1 for named entity recognition can be improved by using a combination of techniques such as word-level and sentence-level features, as well as using a more robust training objective.\n\nQuestion: What are method improvements of F1 for part-of-speech tagging?\n\nAnswer:\n\nF", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " A semicharacter architecture is an architecture that uses a combination of character-based and word-based architectures.\n\nQuestion: What is a word-piece architecture?\n\nAnswer: A word-piece architecture is an architecture that uses a combination of character-based and word-based architectures.\n\nQuestion: What is a word-piece?\n\nAnswer: A word-piece is a single character or a sequence of characters that is used to represent a single unit of meaning.\n\nQuestion: What is a word?\n\nAnswer: A word is a sequence of characters that represents a", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": " The baseline used was the FCE training set.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " 16 languages are explored.\n\nQuestion: which models are explored?\n\nAnswer: 4 models are explored.\n\nQuestion: which lexicons are explored?\n\nAnswer: 2 lexicons are explored.\n\nQuestion: which models are explored?\n\nAnswer: 2 models are explored.\n\nQuestion: which lexicons are explored?\n\nAnswer: 2 lexicons are explored.\n\nQuestion: which models are explored?\n\nAnswer: 4 models are explored.\n\nQuestion: which languages are explored?\n\nAnswer", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " They use the Twitter dataset.\n\nQuestion: What is the objective function?\n\nAnswer: The objective function is to learn unsupervised tweet representations.\n\nQuestion: What are the two models?\n\nAnswer: The two models are PV-DM and PV-DBOW.\n\nQuestion: What are the two features?\n\nAnswer: The two features are attention over contextual words and conceptual tweet embedding.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The difference between the two models is that PV-DM uses attention", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": " The learning models used on the dataset are Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and HybridCNN.\n\nQuestion: What is the difference between the traditional machine learning models and the neural network models?\n\nAnswer: The main difference between traditional machine learning models and neural network models is that the former uses word-level features, while the latter uses character-level features. Additionally, neural network models are trained using backpropagation, while traditional machine learning models are trained using gradient descent.\n\nQuestion: What is the effect of context tweets", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " TF-IDF features and various machine learning classifiers.\n\nQuestion: What is the performance of the model?\n\nAnswer: The model achieved up to 92% accuracy using XGBoost classifier.\n\nQuestion: What is the topic modelling?\n\nAnswer: LDA is used to group the keywords using an intelligent scheme, enabling users to further focus on certain aspects of a document.\n\nQuestion: What is the evaluation metric?\n\nAnswer: The evaluation metrics used are micro and macro averaged F-scores, the harmonic mean of related metrics precision and recall", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\n", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " The training data was translated using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the training data semi-supervised?\n\nAnswer: The training data was semi-supervised by using the DISC BIBREF0 corpus.\n\nQuestion: How was the ensembling performed?\n\nAnswer: The ensembling was performed by averaging the predictions of the eight individual models. Subsequently, the predictions of all individual models were combined into an average. Finally, models were removed from the ensemble in a stepwise manner if the removal increased the average score.\n", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "\n\nAn individual model consists of a set of parameters that are used to calculate the probability of a given sentence.\n\nQuestion: What does a model consist of?\n\nAnswer:\n\nA model consists of a set of parameters that are used to calculate the probability of a given sentence.\n\nQuestion: What does a model consist of?\n\nAnswer:\n\nA model consists of a set of parameters that are used to calculate the probability of a given sentence.\n\nQuestion: What does an individual model consist of?\n\nAnswer:\n\nAn individual model consists of a set of parameters that", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\nQuestion: What model did they use for their system?\n\nAnswer:\n\n", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The political bias of different sources included in the model is included by the labels of the sources. The labels are based on the political bias of the sources.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " The dataset is annotated based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,300 tweets annotated as no evidence of depression (n=6,829 tweets) or evidence of depression (n=2,644 tweets) and further classified the symptom subtype of depressed mood (n=1,010 tweets), disturbed sleep (n=98 tweets), or fatigue or loss of energy (n=427 tweets) using support vector machines", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nThe NCEL approach overall is effective overall. It outperforms the state-of-the-art collective EL methods across five different datasets including ACE2004, AQUAINT and CoNLL. It also achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1.\n\nQuestion: How robust is their NCEL approach to noisy data?\n\nAnswer:\n\nThe NCEL approach is robust to noisy data. It outperforms all baseline methods in both easy and hard cases", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " The ancient Chinese dataset comes from a collection of ancient Chinese articles which are collected from different sources.\n\nQuestion: How is the dataset augmented?\n\nAnswer: The dataset is augmented by merging the original dataset with a set of ancient Chinese articles.\n\nQuestion: How is the dataset used to evaluate the translation models?\n\nAnswer: The dataset is used to evaluate the translation models by using the BLEU score as the evaluation metric.\n\nQuestion: How is the dataset used to evaluate the translation models?\n\nAnswer: The dataset is used to evaluate the translation models by using the B", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?\n\nAnswer:\n\nQuestion: How many layers does the UTCNN model have?", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " Masking words in the decoder is helpful because it helps the decoder to focus on the task of generating a summary. By masking words, the decoder is forced to generate a summary that is similar to the ground truth, which helps to improve the quality of the generated summaries.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " The baseline for this task was a very simple logistic regression classifier with default parameters, where we represented the input instances with a single feature: the length of the sentence. The baseline performance on the test set for the SLC task using BERT BIBREF24 was shown in Tables TABREF33 and TABREF34.\n\nQuestion: What was the baseline for this task?\n\nAnswer: The baseline for this task was a very simple logistic regression classifier with default parameters, where we represented the input instances with a single feature: the length of the sentence. The baseline", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nThe dataset used in this paper is the European network of nature protected sites Natura 2000 dataset.\n\nQuestion: what is the objective of this paper?\n\nAnswer:\n\nThe objective of this paper is to learn geographic location embeddings using Flickr tags, numerical environmental features, and categorical information.\n\nQuestion: what is the main baseline method?\n\nAnswer:\n\nThe main baseline method used in this paper is the Support Vector Machines (SVMs) method.\n\nQuestion: what is the main regression task in this paper?\n\n", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " No, they do not employ their indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "\n\nThe metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n1. Avg. MCC: The average margin of the difference between the predicted and the actual value.\n\n2. Avg. +ve F1 score: The average F1 score of the predicted values and the actual values.\n\n3. Avg. +ve F1 score: The average F1 score of the predicted values and the actual values.\n\n4. Avg. +ve F1 score: The average F1 score of the predicted values and the actual values", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?\n\nAnswer:\n\nQuestion: Which eight NER tasks did they evaluate on?", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "\n\nThe paper uses two clinical datasets: NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the BERT model used in the paper?\n\nAnswer:\n\nThe BERT model is a deep learning model that uses a Transformer-based architecture and transfer learning from very large general-domain multilingual multilingual corpora.\n\nQuestion: What is the transfer learning technique used in the paper?\n\nAnswer:\n\nTransfer learning is a technique that allows a model to learn from a large amount of data without requiring any adaptation or domain-specific feature engineering.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "\n\nGaussian-masked directional multi-head attention works by using a triangular matrix mask to let the self-attention focus on different weights. The triangular matrix is used to alleviate the burden of the fixed bias term and get the prior probability which makes it different from bilinear attention. The distribution of labels in a labeling task is often uneven which makes the output layer often include a fixed bias term for the prior probability of different labels BIBREF27. Bi-affine attention is developed from bilinear attention which has been used in dependency parsing BIBREF29 tool", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " They considered Facebook, Twitter, and Instagram.\n\nQuestion: What types of causal explanations did they consider?\n\nAnswer: They considered causal explanations such as \"because\", \"cuz\", \"because\", \"cause\", \"because\", \"cause\", \"because\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "\n\nThey compare with the following baselines:\n\n1) CRF:\n\n2) LM-LSTM-CRF:\n\n3) LSTM-CNN-CRF:\n\n4) LM-LSTM-CRF:\n\n5) LSTM-CNN-CRF:\n\n6) LM-LSTM-CRF:\n\n7) LSTM-CNN-CRF:\n\n8) LM-LSTM-CRF:\n\n9) LSTM-CNN-CRF:\n\n10) LM", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " Their scores were 0.716, 0.716, 0.716, and 0.716.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " Galatasaray and Fenerbahçe\n\nQuestion: What is the domain of the tweets?\n\nAnswer: Sports-related tweets\n\nQuestion: What are the targets of the stance detection?\n\nAnswer: Favor or Against\n\nQuestion: What are the corresponding annotations?\n\nAnswer: Favor or Against\n\nQuestion: What are the features used in the SVM classifiers?\n\nAnswer: Unigrams, bigrams, and hashtags\n\nQuestion: What are the evaluation metrics used in the SVM classifiers?", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " Their model is compared to the following previous methods:\n\n1. BERT-Base model\n2. BERT-Base model with hypernym expansion\n3. BERT-Base model with hypernym expansion and word list\n4. BERT-Base model with hypernym expansion and Probase taxonomy\n5. BERT-Base model with hypernym expansion and in-house word lists\n\nQuestion: What is the difference between the BERT-Base model and the BERT-Base model with hypernym expansion?\n\nAnswer: The BERT-Base model is a pre-trained language model that is", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The network's baseline features are the features that are extracted from the network's input data.\n\nQuestion: What are the pre-trained features?\n\nAnswer: The pre-trained features are the features that are extracted from the pre-trained models.\n\nQuestion: What is the generalizability of the models developed or referred in this paper?\n\nAnswer: The generalizability of the models developed or referred in this paper is tested on Dataset 3. The F1-score drops down dramatically to 33.05%. In order to understand this finding, we visualize each", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " The corpus consists of 53 case reports, which contain an average number of 156.1 sentences per document, each with an average number of 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers for entities are summarized in Table TABREF29.\n\nQuestion: How many entities are annotated?\n\nAnswer: The corpus contains a total of", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:\n\nQuestion: which chinese datasets were used?\n\nAnswer:", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The training sets of these versions of ELMo compared to the previous ones are 20 times larger.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": " Yes, it is possible to convert a cloze-style questions to a naturally-looking questions. The cloze-style questions are typically constructed by replacing the answer bearing sentence in context with a placeholder. The placeholder can be replaced with a naturally-looking word or phrase, and the question can be rewritten to make it sound more natural.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " Yes, their NER model learns NER from both text and images.\n\nQuestion: Does their NER model learn NER from text and images?\n\nAnswer: Yes, their NER model learns NER from both text and images.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes, their NER model learns NER from both text and images.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes, their NER model learns NER from both", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " The dataset contains 64 sentences.\n\nQuestion: How many unique words does the dataset contain?\n\nAnswer: The dataset contains 16225 unique words.\n\nQuestion: How many entities does the dataset contain?\n\nAnswer: The dataset contains 16225 unique entities.\n\nQuestion: How many POS tags does the dataset contain?\n\nAnswer: The dataset contains 16225 unique POS tags.\n\nQuestion: How many words does the dataset contain?\n\nAnswer: The dataset contains 30225 words.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " The authors provide a survey among engineers, which shows that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques.\n\nQuestion: How do the authors evidence the claim that NeuronBlocks can achieve reliable and competitive results on various NLP tasks with productivity greatly improved?\n\nAnswer: The authors provide extensive experiments on CoNLL-2003 Engl NER dataset, GLUE benchmark dataset, WikiQA corpus dataset, and Domain Classification dataset to show that the models built with NeuronBlocks can achieve competitive or even better results with simple model", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "\n\nQuestion: What cognitive features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion: What traditional linguistics features did they use?\n\nAnswer:\n\nQuestion:", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " Yes, they evaluate only on English datasets.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: Yes, they evaluate only on English datasets.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: Yes, they evaluate only on English datasets.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: Yes, they evaluate only on English datasets.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: Yes, they evaluate only on English datasets.\n\nQuestion: Do they evaluate only on English datasets?\n\nAn", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " 0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.286", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " The experiments conducted are:\n\n1. BackTrans: In BIBREF7 , authors propose a model using machine translation in order to preserve the meaning of the sentence while reducing stylistic properties.\n2. Unpaired: In BIBREF10 , researchers implement a method to remove emotional words and add desired sentiment controlled by reinforcement learning.\n3. CrossAlign: In BIBREF37 , authors leverage refined alignment of latent representations to perform style transfer and a cross-aligned auto-encoder is implemented.\n4. CPTG: An interpolated reconstruction loss is introduced in BIB", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": " On SimpleQuestions BIBREF2 and WebQSP BIBREF25.\n\nQuestion: What is the main difference between their model and the previous state of the art?\n\nAnswer: The main difference between their model and the previous state of the art is that they use a hierarchical matching between questions and KB relations.\n\nQuestion: What is the main difference between their model and the previous state of the art?\n\nAnswer: The main difference between their model and the previous state of the art is that their model uses a hierarchical matching between questions and KB relations.\n", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": " The hyperparameters varied in the experiments on the four tasks were:\n\n1. Number of clusters\n2. Number of features\n3. Number of clusters\n4. Number of clusters\n\nThe hyperparameters were varied in the experiments on the four tasks were:\n\n1. Number of clusters\n2. Number of features\n3. Number of clusters\n4. Number of clusters\n\nThe hyperparameters varied in the experiments on the four tasks were:\n\n1. Number of clusters\n2. Number of features\n3. Number of clusters\n4. Number of clusters\n\nThe hyperparameters varied in the experiments", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " NLP tasks include text classification, sentiment analysis, topic modeling, and text generation.\n\nQuestion: What NLP tasks do they consider?\n\nAnswer: NLP tasks include text classification, sentiment analysis, topic modeling, and text generation.\n\nQuestion: What NLP tasks do they consider?\n\nAnswer: NLP tasks include text classification, sentiment analysis, topic modeling, and text generation.\n\nQuestion: What NLP tasks do they consider?\n\nAnswer: NLP tasks include text classification, sentiment analysis, topic modeling, and text generation.\n\nQuestion: What NLP tasks do", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "\n\nThey compare to MLP, Eusboost, and MWMOTE.\n\nQuestion: What are the main differences between the models/frameworks?\n\nAnswer:\n\nThe main differences between the models/frameworks are:\n\n1. The models/frameworks use different data representation.\n2. The models/frameworks use different training data.\n3. The models/frameworks use different feature extraction methods.\n4. The models/frameworks use different feature selection methods.\n\nQuestion: What are the main differences between the models/frameworks?\n\nAnswer", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "\n\nQuestion: What is the difference between the Markov model and the DMV model?\n\nAnswer:\n\nQuestion: What is the difference between the Markov model and the neural HMM?\n\nAnswer:\n\nQuestion: What is the difference between the Markov model and the Gaussian DMV model?\n\nAnswer:\n\nQuestion: What is the difference between the Markov model and the Gaussian DMV model?\n\nAnswer:\n\nQuestion: What is the difference between the Markov model and the DMV model?\n\nAnswer:\n\nQuestion: What is", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
