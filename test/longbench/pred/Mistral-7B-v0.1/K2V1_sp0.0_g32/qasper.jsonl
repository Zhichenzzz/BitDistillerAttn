{"pred": "\n\nQuestion: How is the ground truth for fake news is established?\n\nAnswer: unanswerable\n\nQuestion: How is the ground truth for fake news.\n\nAnswer: unanswerable\n\nQuestion: How is the ground truth for fake news.\n\nQuestion: How is the ground truth for fake news.\n\nAnswer: How is the ground truth for fake news.\n bekan\n\nAnswer: How is the ground truth for fake news.\n\nAnswer: How is the ground truth for fake news.\n\nAnswer: How is the ground truth for fake news.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach.\n\nQuestion: What is the GhostVLAD approach?\n\nAnswer: The GhostVLAD approach is a pooling approach.\n\nQuestion: What is the GhostVLAD approach?\n\nAnswer: The GhostVLAD is a yes/no.\n\nQuestion: What is the GhostVLAD approach?\n\nAnswer: The GhostVLAD is a yes/no.\n\nQuestion: What is the GhostVLAD approach?\n\nAnswer: The GhostVLAD is the best model.\n\nQuestion: What is the", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "0.68\n\nQuestion: What is the model?\n\nAnswer:\n\nQuestion: What is the model?\n\nAnswer:\n\nQuestion: What is the model?\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\nAnswer\n\n", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "\n\nQuestion: What is the best model (B-M).\n\nQuestion: What is the best model (B-M)\n\nAnswer:\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B-M)\n\nQuestion: What is the best model (B", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "\n\nQuestion: What additional features and context are proposed.\n\nQuestion: What additional features and context are proposed.\n\nAnswer: What additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.\n\nAnswer: The additional features and context are proposed.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "\n\nQuestion: Do the hashtag and SemEval datasets contain only English data.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data.\n bekan, the SemEval datasets.\n\nQuestion: Do the hashtag and SemEval datasets.\n\nAnswer: The hashtag and SemEval datasets.\n\nAnswer: The hashtag and SemEval datasets.\n\nAnswer: The hashtag and SemEval datasets.\n\nAnswer: The hashtag and", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The evaluation is based on the concept map.\n\nQuestion: What is the average divergence between the documents.\n\nQuestion: What is the average divergence between the documents.\n\nQuestion: What is the average divergence between the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What is the average of the documents.\n\nQuestion: What", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The proposed approach is different from other WSD approaches employing word embeddings.\n\nQuestion: How does this approach compare to other WSD approaches employing.\n\nQuestion: How does this approach compares to other WSD approaches.\n\nAnswer: How does this approach compares to other WSD approaches.\n\nAnswer: How does this approach compares to other WSD approaches.\n\nQuestion: How does this approach compares to other WSD approaches.\n\nQuestion: How does this approach compares to other WSD approach.\n\nAnswer: How does this approach to the other W", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The datasets used for evaluation are CNN/DailyMail, NYT, NYT, and XSum.\n\nQuestion: What are the datasets used for evaluation.\n\nQuestion: What are the datasets used for evaluation.\n\nAnswer: What are the datasets used for evaluation.\n\nQuestion: What are the datasets used for evaluation.\n\nAnswer: What are the datasets used for evaluation.\n\nQuestion: What are the datasets used for evaluation.\n\nQuestion: What are the datasets used for evaluation.\n\nQuestion: What are the datasets used for evaluation.\n\nQuestion: What are the datasets", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "\n\nQuestion: What is the best performing model?\n\nAnswerable\n\nQuestion: How does their ensemble method work?\n\nQuestion: How does their ensemble method?\n\nAnswer: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?\n\nQuestion: How does their ensemble method?", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The source of the datasets are from the EmotionLines.\n\nQuestion: What are the characteristics of the six main characters in the dialogue.\n\nAnswer: The source of the six characters are the six characters.\n\nQuestion: What are the six characters are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters.\n\nQuestion: What are the six characters", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "The IMDb dataset.\n\nQuestion: What is the best model for the best model for SA.\n\nQuestion: What is the best model for SA.\n\nAnswer: The best model for the SA.\n\nQuestion: What is the best model for NER.\n\nAnswer: The best model for NER.\n\nQuestion: What is the best model for NER.\n\nAnswer: The best model for NER.\n\nQuestion: What is the best model for NER.\n\nQuestion: What is the best model for NER.\n\nQuestion: What is", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "English\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: what language does this paper focus on?\n\nAnswer: unanswerable\n\nAnswer: what language does this paper focus on?\n\nAnswer: what language does this paper focus on the paper.\n\nAnswer: what language does this paper focus on the paper.\n\nAnswer: what language does this paper focus on the paper.\n\nAnswer: what language does this paper focus on the paper.\n\nAnswer: what language does paper focus on the paper.\n\nAnswer: what", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "The proposed system achieves a F1 value of +1.08 F1.\n\nQuestion: What is the proposed system?\n\nAnswer: The proposed system is a neural network.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the proposed system.\n\nQuestion: What is the", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nQuestion: Did they experiment with this new dataset?\n\nQuestion: Did they experiment with this new dataset?\n\nAnswer: They experimented with this new dataset.\n\nQuestion: Did they experiment with this new dataset.\n\nAnswer: They experiment with this new dataset.\n\nQuestion: Did they experiment with this new dataset.\n\nAnswer: They experiment with this new dataset.\n\nQuestion: Did they experiment with this new dataset.\n\nAnswer: They did this new dataset.\n\nQuestion: Did they experiment with this new dataset.\n\nQuestion: Did they experiment", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The best performance?\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector.\n\nAnswer: Which stock market sector.\n\nAnswer: Which stock market sector.\n\nAnswer: Which market sector.\n\nAnswer: Which market sector.\n\nAnswer: Which market sector.\n\nAnswer: market sector.\n\nAnswer: market.\n\nAnswer: market.\n\nAnswer: market.\n\nAnswer: market.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT model, Transformer, and Transformer.\n\nQuestion: what NMT models did they compare with?\n\nAnswer: SMT, RNN-based NMT, and Transformer.\n\nQuestion: what NMT models did they compare with RNN-based NMT.\n\nAnswer:\n\nQuestion: what NMT models did they compare with RNN-based.\n\nAnswer:\n\nQuestion: what NMT models did they compare with SMT.\n\nAnswer:\n\nQuestion: what is the NMT models.\n\nQuestion:", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "\n\nQuestion: What are the three regularization terms?\n\nAnswer: What are the three regularization terms?\n\nAnswer: What are the three regularization terms?\n\nAnswer: What are the three regularization terms?\n\nAnswer: What are the three regularization terms.\n\nQuestion: What are the three regularization terms.\n\nAnswer: What are the three regularization terms.\n\nAnswer: What are the three regularization terms.\n\nAnswer: What are the three regularization terms.\n\nAnswer: What are the three regularization terms.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "\n\nQuestion: What is the system testers.\n\nQuestion: What is the system testers.\n\nQuestion: What is the system.\n\nQuestion: What is the system tes.\n\nQuestion: What is the system tes.\n\nQuestion: What is the system tes.\n\nQuestion: What is the system.\n\nQuestion: What is the system.\n\nQuestion: What is the system.\n\nQuestion: What is the system.\n\nQuestion: What is the system.\n\nQuestion: What is the system.\n\nQuestion: What is the system", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "\n\nQuestion: What is the best performance.\n\nAnswer:\n\nQuestion: What is the best performance.\n\nAnswer:\n\nQuestion: What is the best performance.\n\nAnswer:\n\nQuestion: What is the best performance.\n\nQuestion:\n\nQuestion: What is the best performance.\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "\n\nQuestion: what was the baseline?\n\nAnswer: the baseline?\n\nQuestion: what was the baseline?\n\nAnswer: the baseline?\n\nAnswer: the baseline.\n\nQuestion: what was the baseline?\n\nAnswer: the baseline.\n\nQuestion: the baseline.\n\nQuestion: the baseline.\n\nAnswer: the baseline.\n\nQuestion: the baseline.\n\nAnswer: the baseline.\n\nAnswer: the baseline.\n\nQuestion: the baseline.\n\nQuestion: the baseline.\n\nQuestion: the baseline.\n\nQuestion", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "\n\nQuestion: How does their model improves interpretability\n\nAnswer:\n\nQuestion: How does their model improves\n\nAnswer:\n\nAnswerable\n\nQuestion: How does their model interpretability\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswer\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\nAnswerable\n\n", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "SVM with unigram, bigram, trigram, and trigram, and the average word embedding.\n\nQuestion: What are the baselines?\n\nQuestion: What are the baselines.\n\nAnswer: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\nQuestion: What are the baselines.\n\n", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The attention module is pretrained on the ASR encoder.\n\nQuestion: What is the attention module pretrained on the ASR.\n\nAnswer: The attention module is pretrained on the ASR.\n\nQuestion: What is the attention module.\n\nAnswer: The attention module is pretrained on the ASR.\n\nQuestion: What is the attention module.\n\nQuestion: What is the attention module.\n\nQuestion: What is the attention module.\n\nQuestion: What is the attention module.\n\nQuestion: What is the attention module.\n\nQuestion: What is the", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "\n\nQuestion: What is the baseline system?\n\nAnswer:\n\nQuestion: What is the baseline system.\n\nAnswer:\n\nQuestion: What architecture does the baseline system?\n\nAnswer:\n\nQuestion: What architecture does the baseline system.\n\nQuestion: What architecture does the baseline system.\n\nAnswer: What architecture.\n\nQuestion: What architecture.\n\nAnswer: What architecture.\n\nQuestion: What architecture.\n\nAnswer: What architecture.\n\nAnswer: What is the architecture.\n\nAnswer: What is the architecture.\n\n", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "\n\nQuestion: What kind of stylistic features are obtained.\n\nQuestion: What kind of stylistic features are obtained.\n\nQuestion: What kind of stylistic features are obtained.\n\nQuestion: What kind of stylist features are obtained.\n\nQuestion: What kind of stylist features are obtained.\n\nQuestion: What kind of the stylist features.\n\nQuestion: What kind of the stylist features.\n\nQuestion: What kind of the stylist features.\n\nQuestion: What kind of the stylist features.\n\nQuestion: What kind of the stylist of the", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The evaluation metrics are XNLI and UD parsing.\n\nQuestion: What is the performance of the model is used for evaluation.\n\nAnswer: What is the performance of the model.\n\nQuestion: What is the performance of the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer: What is the model.\n\nAnswer", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nAnswer: what were the baselines?\n\nQuestion: what were the baselines?\n\nAnswer:\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\nQuestion: what were the baselines?\n\n", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nQuestion: What is the", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "22,880\n\nQuestion: What is the best way to predict the industry of a user.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many users do they look at the industry.\n\nQuestion: How many", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Yes\n\nQuestion: Is the model performance on the new task.\n\nQuestion: Is the model performance on the task.\n\nQuestion: Is WordNet is useful for taxonomic reasoning for this task.\n\nQuestion: Is WordNet is useful for taxonomic for this task.\n\nQuestion: Is WordNet is useful for this task.\n\nQuestion: Is WordNet is useful for this task.\n\nQuestion: Is WordNet is useful for this task.\n\nQuestion: Is WordNet is useful for this task.\n\nQuestion: Is WordNet is useful for this task.\n\n", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The improvement in performance for Estonian is 0.000.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement for Estonian.\n\nQuestion: What is the improvement in performance for Estonian.\n\nQuestion: What is the improvement for", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nQuestion: How much data is needed to train the task-specific encoder.\n\nQuestion: How much data is needed to train the encoder.\n\nQuestion: How much data is needed to train the encoder.\n\nQuestion: How much data is needed to train the encoder.\n\nQuestion: How much data is needed to train the encoder.\n\nQuestion: How much data is needed to train the encoder.\n\nQuestion: How much data is needed to train the encoder.\n\n", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "\n\nQuestion: What is the difference between the entmax and sparsemax.\n\nAnswer: What is the difference between the entmax and the sparsemax.\n\nQuestion: What is the difference between the entmax and the sparsemax.\n bekan the difference between the entmax.\n\nAnswer: What is the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "\n\nQuestion: What is the linguistic characteristics of the dataset.\n\nAnswer: What is the linguistic characteristics of the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.\n\nQuestion: What is the dataset.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "No, this paper is introducing a supervised method.\n\nQuestion: What is the best way to detect spammer detection.\n\nAnswer: No.\n\nQuestion: What is the best way to detect spam detection.\n\nAnswer: The best way to detect spam detection.\n\nAnswer: The best way to detect spam detection.\n\nAnswer: The best way to detect spam.\n\nAnswer: The best way to detect spam.\n\nAnswer: The best way to detect spam.\n\nAnswer: The best way to detect spam", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "\n\nQuestion: what is the best model?\n\nAnswer:\n\nQuestion: what is the best model?\n\nAnswer:\n\nQuestion: which lstm model?\n\nAnswer:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Nguni, Sotho, Afrika, and Tshivenda.\n\nQuestion: Which languages are similar to each other?\n\nAnswer: Nguni, Afrika, Afrika, and Tshivenda.\n\nQuestion: Which languages are similar to each other.\n\nAnswer: Which languages are similar to each other.\n\nAnswer: Which languages are similar to each other.\n\nQuestion: Which languages are similar to each other.\n\nAnswer: The Nguni, Afrika, and the Nguni, and the Nguns.\n\nQuestion", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The efficiency of the model is evaluated by the retention rate of the model.\n\nQuestion: How are models evaluated in this human-machine communication game.\n\nAnswer: The model is evaluated by the retention rate of the retention.\n\nAnswer: The model is evaluated by the retention.\n\nAnswer: The model is evaluated by the retention.\n\nAnswer: The model is evaluated by the retention.\n\nAnswer: The model is evaluated by the retention.\n\nAnswer: The model is evaluated by the retention.\n\nAnswer: The model", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "\n\nQuestion: How were the human judgements assembled?\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the human judgements.\n\nAnswer: How were the", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "\n\nQuestion: What background do they have?\n\nQuestion: What background do they have?\n\nAnswer:\n\nQuestion: What is the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the background of the", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.\n\nQuestion: Do they test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: Do they test their framework performance on commonly used language pairs.\n\nAnswer: Do they test their framework performance on commonly used language pairs.\n\nQuestion: Do they test their framework performance on commonly used language pairs.\n\nAnswer: Do they test their framework performance on commonly used language pairs.\n\nAnswer: Do they test their framework performance.\n\nAnswer: Do they test their framework performance.\n\nAnswer: Do they test their framework performance", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "NeuronBlocks includes a suite of reusable and standard and reusable and reusable and reusable.\n\nQuestion: What is the difference between NeuronBlocks?\n\nAnswer: NeuronBlocks is a DNN toolkit.\n\nAnswer: What is the difference between NeuronBlocks.\n\nAnswer: What is the difference between NeuronBlocks.\n\nAnswer: NeuronBlocks.\n\nAnswer: NeuronBlocks.\n\nAnswer: NeuronBlocks.\n\nAnswer: NeuronBlocks.\n\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "\n\nQuestion: How large is their data set?\n\nQuestion: How large is their data set.\n\nQuestion: How large is their data set.\n\nQuestion: How large is their data set?\n\nQuestion: How large is their data set?\n\nQuestion: How large is their data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set?\n\nQuestion: How large data set", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The source domain is the source domain, and the target domain is the target domain.\n\nQuestion: What is the source domain?\n\nAnswer: The source domain is the source domain.\n\nQuestion: What is the source domain?\n\nAnswer: The source domain.\n\nQuestion: What is the target domain?\n\nQuestion: What is the target domain?\n\nQuestion: The target domain.\n\nQuestion: The target domain.\n\nQuestion: The target domain.\n\nQuestion: The target domain.\n\nQuestion: The target domain.\n\nQuestion: The target domain.\n", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "\n\nQuestion: what previous RNN models they compare with?\n\nQuestion: what previous RNN models they compare with.\n\nQuestion: what previous RNN models they compare with.\n\nQuestion: what previous RNN models they compare with.\n\nQuestion: what previous RNN models they compare with.\n\nQuestion: what previous RNN models they compare with.\n\nQuestion: what previous RNNs they compare with.\n\nQuestion: what previous RNNs they compare with.\n\nQuestion: what previous RNNs they compare with.\n\nQuestion: what previous RNNs they compare", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "They test their method on the task of predicting hashtags for a held-out-of-out-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "\n\nQuestion: What were the baselines?\n\nAnswer:\n\nQuestion: What were the baselines?\n\nAnswer:\n\nAnswer:\n\nQuestion: What were the baselines?\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAn", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nAnswer: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n\nQuestion: What is the best way to answer the question.\n", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "Spanish, Finnish, English, Finnish, and Finnish.\n\nQuestion: What is the main goal of the article.\n\nQuestion: What is the main goal of the article.\n\nAnswer: The main goal of the article.\n\nQuestion: What is the main goal of the article.\n\nAnswer: What is the main goal of the article.\n\nQuestion: What is the main goal of the article.\n\nAnswer: What is the main goal of the article.\n\nAnswer: What is the main goal of the article.\n\nAnswer: What is the", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "\n\nQuestion: Was PolyReponse evaluated against some baseline?\n\nAnswer: Yes\n\nQuestion: Was PolyReponse evaluated against some baseline?\n\nAnswer: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nAnswer: PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion: Was PolyReponse\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "\n\nQuestion: what datasets did they use?\n\nQuestion: what datasets did they use?\n\nAnswer: what datasets did they use?\n\nAnswer: what datasets they use.\n\nQuestion: what datasets did they use?\n\nAnswer: what datasets did they use.\n\nQuestion: what datasets did they use.\n\nQuestion: what datasets did they use?\n\nAnswer:\n\nQuestion: what datasets did they use.\n\nQuestion: what datasets did they use.\n\nQuestion: what datasets did they use.\n\nQuestion: what datasets did they use.\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "\n\nQuestion: How do they obtain psychological dimensions of people?\n\nAnswer: unanswerable\n\nQuestion: How do they obtain psychological dimensions of people.\n\nQuestion: How do they obtain psychological dimensions of people.\n\nAnswer: unanswerable.\n\nQuestion: How do they obtain psychological dimensions of people.\n\nAnswer: How do they obtain psychological dimensions of people.\n\nQuestion: How do they obtain psychological dimensions of people.\n\nAnswer: How do they obtain psychological dimensions of people.\n\nQuestion: How do they obtain psychological dimensions of people.\n\nAnswer:", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify the argument components.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "1,873\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nAnswer: 14,000\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?\n\nQuestion: How large is the Twitter dataset?", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes.\n\nQuestion: What is the model with gated orthogonalization.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\nQuestion: Do they use pretrained embeddings.\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "The quality of the data is empirically evaluated by the quality of the data.\n\nQuestion: How is the quality of the data is empirically evaluated.\n\nQuestion: How is the quality of the data is empirically evaluated.\n\nAnswer: How is the quality of the data.\n\nAnswer: How is the quality of the data.\n\nQuestion: How is the quality of the data.\n\nAnswer: How is the quality of the data.\n\nAnswer: How is the quality of the data.\n\nAnswer: How is the quality of the data.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The model is applied to Wikipedia and Reddit.\n\nQuestion: What is the model is applied to the model is applied to.\n\nQuestion: What is the model is applied to the model is applied to the model is applied to the model.\n\nQuestion: What are the model is applied to the model is applied to the model.\n\nQuestion: What is the model is applied to the model is the model.\n\nQuestion: What is the model is applied to the model is the model.\n\nQuestion: What is applied to the model to the model.\n\nQuestion: What is applied to the model is", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "4-grams\n\nQuestion: What is the length of the ngrams of the table.\n\nQuestion: What is the length of the table-to-to-to-to-table-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "English, French, German, Spanish, French, Chinese, English, French, German, Russian, English, French, English, English, French, English, English, and English.\n\nQuestion: What are the 12 languages covered.\n\nAnswer: The 12 languages covered are typologically diverse and under-resour languages.\n\nQuestion: What are the 12 languages.\n\nAnswer: The 12 languages are the languages.\n\nQuestion: What are the 12 languages.\n\nQuestion: What are the 12 languages.\n\nQuestion: What are", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "\n\nQuestion: What is their definition of tweets going viral.\n\nQuestion: What is their definition of tweets going viral.\n\nQuestion: What is their definition of tweets going viral.\n\nQuestion: What is their definition of tweets viral.\n\nQuestion: What is their definition of tweets viral.\n\nQuestion: What is their definition of tweets viral.\n\nQuestion: What is their definition of tweets viral.\n\nQuestion: What is their definition of tweets viral.\n\nQuestion: What is their definition of tweets viral", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "2.11 BIBREF27\n\nQuestion: what is the SOTA model?\n\nAnswer: by how much did their model improve?\n\nAnswer: 2.11 BIBREF28 , BIBREF29 , BIBREF0 , BIBREF10 , BIBREF20 , BIBREF20 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF0 , BIBREF", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "No.\n\nQuestion: What is the main goal of this work.\n\nQuestion: What is the main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The main goal of the pipeline components.\n\nAnswer: The goal of", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "They use a dual RNN to encode the audio and text sequences.\n\nQuestion: What is the best-performance of the MDRE model.\n\nAnswer: The MDRE model is a multimodal sequences.\n\nAnswer: The MDRE model is a multimodal.\n\nQuestion: What is the MDRE model.\n\nAnswer: The MDRE model is a multimodal.\n\nAnswer: The MDRE model is a multimodal.\n\nQuestion: How do they combine audio and text sequences.\n\nAnswer: The MDRE", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The benchmark dataset is the Honeypot dataset. The quality of the benchmark dataset is high.\n\nQuestion: What is the benchmark dataset?\n\nAnswer: The benchmark dataset is high.\n\nQuestion: What is the benchmark dataset is high.\n\nAnswer: The benchmark dataset is high.\n\nQuestion: What is the benchmark dataset.\n\nAnswer: The benchmark dataset is high.\n\nQuestion: What is the benchmark dataset is high.\n\nAnswer: The benchmark dataset is high.\n\nQuestion: What is the benchmark dataset", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The data is collected by crowdsourcing.\n\nQuestion: what is the database is the largest public text-dependent and text-independent speech recognition.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is the largest database.\n\nQuestion: what is", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "700\n\nQuestion: how many humans evaluated the results?\n\nQuestion: how many humans evaluated the results?\n\nQuestion: how many humans evaluated the results?\n\nQuestion: how many humans evaluated the results?\n\nQuestion: how many humans evaluated the results?\n\nQuestion: how many humans evaluated the results.\n\nQuestion: how many humans evaluated the results.\n\nQuestion: how many humans evaluated the results.\n\nQuestion: how many humans evaluated the results.\n\nQuestion: how many humans evaluated the results.\n\nQuestion: how many humans evaluated the results.\n\nQuestion", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN, BERT, BERT, CNN, CNN, CNN, BERT, BERT, CNN, CNN, CNN, CNN, CNN, CNN, BERT, BERT, CNN, CNN, BERT, CNN, BERT, CNN, BERT, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN, CNN", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "\n\nQuestion: what was the baseline?\n\nAnswer: what was the baseline?\n\nQuestion: what was the baseline?\n\nQuestion: what was the baseline?\n\nAnswer: what was the baseline?\n\nQuestion: what was the baseline?\n\nAnswer: what was the baseline?\n\nQuestion: what was the baseline?\n\nAnswer: what was the baseline?\n\nQuestion: what was the baseline?\n\nQuestion: what was the baseline?\n\nAnswer: what was the baseline?\n\nQuestion: what was the baseline?\n\nQuestion: what was", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "\n\nQuestion: Do they report results only on English data.\n\nQuestion: Do they report results only on English data.\n\nQuestion: Do they report results only on English data.\n\nQuestion: Do they report results only on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\nQuestion: What is the best method for RQE?\n\n", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "\n\nQuestion: What is the baseline system.\n\nQuestion: What is the baseline system.\n\nQuestion: What is the baseline system.\n\nAnswer: What architecture does the decoder has a multilingual training, the decoder has a multilingual training.\n\nQuestion: What is the decoder has a multilingual training.\n\nAnswer: What is the decoder.\n\nAnswer: What is the decoder.\n\nAnswer: What is the decoder.\n\nAnswer: What is the decoder.\n\nAnswer: What is the decoder", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the paper explore the extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from electronic health records.\n\nQuestion: Does the", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "\n\nQuestion: What is the correlation between the human judgments.\n\nAnswer: The paper is a retrofitting vector-faith.\n\nQuestion: What is the second-order co-fait.\n\nAnswer: The paper.\n\nQuestion: The paper is the paper.\n\nAnswer: The paper is the paper.\n\nAnswer: the paper.\n\nAnswer: the paper.\n\nQuestion: the paper.\n\nAnswer: the paper.\n\nQuestion: the paper.\n\nQuestion: the paper.\n\nQuestion: the paper.\n", "answers": ["Skipgram, CBOW", "integrated vector-res, vector-faith, Skipgram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The best performing model is the ensemble of (r19)\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the best performing model.\n\nQuestion: What is best performing model among author's submissions.\n\nAnswer: The best performing model is the best performing model.\n\nQuestion: What is best performing model is the best performing model.\n\nAnswer: What is best performing model is the best performing model.\n\nQuestion: What is best performing model is the best performing model.\n", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "\n\nQuestion: Do the authors hypothe that humans' robustness to noise is due to noise.\n\nQuestion: Do the authors hypothese robustness to noise.\n\nAnswer: Do the authors hypothese the robustness.\n\nQuestion: Do the authors hypothese the robustness.\n\nAnswer: Do the authors hypothese the robustness.\n\nAnswer: Do the authors hypothese the robustness.\n\nAnswer: Do the authors the robustness.\n\nAnswer: Do the authors the robustness.\n\nAnswer: Do the authors the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The model for painting embedding is a sequence-to-sequence model.\n\nQuestion: What is the average content score is 3.7.\n\nAnswer: What is the average content score.\n\nAnswer: The average content score is 3.9.\n\nAnswer: What is the average content score is 3.9.\n\nAnswer: What is the average content score is 3.9.\n\nAnswer: What is the average content score is 3.9.\n\nAnswer: What is the average content score is 3.9.\n\nAn", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "\n\nQuestion: How do they match words before reordering them.\n\nAnswer:\n\nQuestion: How do they match words before reordering.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "\n\nQuestion: How do they obtain the new context representation?\n\nAnswer:\n\nQuestion: How do they obtain the new context representation?\n\nAnswer: How do they obtain the new context representation.\n\nAnswer: How do they obtain the new context representation.\n\nAnswer:\n\nQuestion: How do they obtain the new context representation.\n\nAnswer: How do they obtain the new context.\n\nAnswer: How do they obtain the new context representation.\n\nAnswer: How do they obtain the new context representation.\n\nAnswer: How do they obtain the", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The experts were legal experts.\n\nQuestion: What is the best performing model.\n\nAnswer: The best performing model.\n\nAnswer: Who were the experts used for annotation.\n\nQuestion: What is the best performing model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer: The best model.\n\nAnswer", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer:\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: How much higher quality is the annotated data.\n\nQuestion: How much higher quality is the data.\n\nQuestion: How much higher quality is the data.\n\nQuestion: How much higher quality is the data.\n\nQuestion: How much higher quality is the data.\n\nQuestion: How much higher quality is the data.\n\nQuestion: How much higher quality is the data.\n\nQuestion", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "\n\nQuestion: On top of BERT does the RNN layer work better or the transformer layer?\n\nAnswer: unanswerable\n\nQuestion: On top of BERT does the RNN layer work better or the transformer layer.\n\nQuestion: On top of BERT does the RNN layer work better or the transformer.\n\nQuestion: On top of BERT.\n\nQuestion: On top of BERT.\n\nQuestion: On top of the RNN layer.\n\nQuestion: On top of BERT.\n\nQuestion: On top of BERT.\n\nQuestion", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "The dataset is the Multi30K dataset.\n\nQuestion: What is the name of the source degradation setup.\n\nAnswer: The dataset is the Multi30K dataset.\n\nQuestion: What is the dataset is the Multi30K dataset.\n\nAnswer: The dataset is the Multi30K dataset.\n\nAnswer: What dataset is the Multi30K dataset.\n\nAnswer: What dataset is the dataset.\n\nQuestion: What dataset is the dataset.\n\nAnswer: What is the dataset is the dataset.\n\nAnswer: What dataset", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "\n\nQuestion: What cyberbullying topics did they address?\n\nQuestion: What cyberbullyng is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a social network.\n\nQuestion: What cyberbully is a", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Logistic regression and Multilayer perceptron.\n\nQuestion: What is the difference between the two classifiers.\n\nQuestion: What is the difference between the keyword discovery and expectation estimation.\n\nQuestion: What type of classifiers are used.\n\nQuestion: What type of classifiers are used.\n\nQuestion: What type of the classifiers are used.\n\nQuestion: What type of the classifiers are used.\n\nQuestion: What type of the classifiers are used.\n\nQuestion: What type of the classifiers are used.\n\nQuestion: What type of the class", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: the best performing model.\n\nQuestion: the best performing model.\n\nQuestion: the best model.\n\nQuestion: the best model.\n\nQuestion: the best model.\n\nQuestion: the best model.\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The experiments are performed on the SQuAD dataset.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n\nQuestion: What is the main results of the article.\n Question: What is the main results of the article.\n", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nQuestion: What is the impact of gender bias in ASR performance?\n\nQuestion: What is the impact of gender bias in ASR performance?\n\nQuestion: How big is imbalance in analyzed corpora.\n\nQuestion: How big is imbalance in analyzed corpora.\n\nQuestion: What is the impact of gender bias.\n\nQuestion: How big is the impact of gender bias.\n\nQuestion: How big is the impact of gender bias.\n\nQuestion: How big is the impact of gender bias.\n\nQuestion: How big is the impact of gender bias.\n\nQuestion:", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\n\nAnswer: No\n\nAn", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "\n\nQuestion: What is the Transformer model is compared to?\n\nAnswer:\n\nQuestion: What is the Transformer is a model is compared to?\n\nAnswer:\n\nQuestion: What is the Transformer is a model is a model.\n\nAnswer:\n\nQuestion: What is the Transformer is a model.\n\nAnswer:\n\nQuestion: What is the Transformer is a model is a model.\n\nQuestion: What is the Transformer is a model.\n\nQuestion: What is the Transformer is a model.\n\nQuestion: What is the", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "NLP toolkits\n\nQuestion: What is the task of entity-level sentiment analysis.\n\nAnswer: What is the toolkit?\n\nAnswer: The toolkit is a toolkit.\n\nQuestion: What is the toolkit?\n\nAnswer: The toolkit is a toolkit.\n\nAnswer: What is the toolkit.\n\nAnswer: What is the toolkit is the toolkit.\n\nAnswer: What is the toolkit.\n\nAnswer: What is the toolkit.\n\nAnswer: What is the toolkit.\n\n", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The IMDb dataset\n\nQuestion: What is the hidden state of the QRNN?\n\nAnswer: The QRNN is a deep learning.\n\nQuestion: What is the hidden state of the QRNN.\n\nAnswer: The QRNN is a QRNN.\n\nQuestion: What is the QRNN is a QRNN.\n\nAnswer: The QRNN is a QRNN is a QRNN.\n\nQuestion: What is the QRNN is a QRNN is a QRNN.\n\nAnswer: The Q", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "\n\nQuestion: what is the proposed method?\n\nAnswer: what are the existing approaches?\n\nAnswer: what is the proposed method?\n\nAnswer: what is the proposed method?\n\nAnswer: what is the proposed method?\n\nAnswer: what is the proposed method?\n\nAnswer: what is the method?\n\nAnswer: what is the method?\n\nAnswer: what is the method?\n\nAnswer: what is the method?\n\nAnswer: what is the method?\n\nAnswer: what is the method?\n\nAn", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "\n\nQuestion: What is the best model for CSAT\n\nAnswer: unanswerable\n\nQuestion: What is the best model for CSAT\n\nAnswer:\n\nQuestion: What is the best model for CSAT\n\nAnswer:\n\nQuestion: What is the best model for CSAT\n\nAnswer: What is the best model for CSAT\n\nAnswer:\n\nQuestion: What is the best model for CSAT\n\nAnswer: What is the best model for CSAT\n\nAnswer:\n\nQuestion: What is the best model for CSAT\n\n", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "\n\nQuestion: What is the proposed qualitative annotation schema.\n\nQuestion: How does proposed qualitative annotation schema looks like?\n\nAnswer: How does proposed qualitative annotation schema looks like.\n\nQuestion: How does proposed qualitative annotation schema looks like.\n\nAnswer: The proposed qualitative annotation schema looks like.\n\nQuestion: How does proposed qualitative annotation schema looks like.\n\nQuestion: How does the proposed qualitative annotation schema looks like.\n\nQuestion: How does the proposed qualitative annotation schema looks like.\n\nQuestion: How does", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The invertibility condition is a condition that the Jacobian determinant.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Yes.\n\nQuestion: Were any of these tasks were evaluated in any previous work?\n\nAnswer: Were any of these tasks evaluated in any previous work.\n\nAnswer: yes.\n\nQuestion: Were any of these tasks evaluated in any previous work.\n\nAnswer: any of these tasks evaluated in previous work.\n\nAnswer: any of these tasks evaluated in any previous work.\n\nAnswer: any of these tasks evaluated in the previous work.\n\nAnswer: any of these tasks evaluated in previous work.\n\nAnswer: any of these tasks evaluated in the previous work", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "The models are SVMs, BiLSTM, BiLSTM, and CNN.\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nAnswer: The best model is a CNN.\n\nQuestion: What is the best model?\n\nAnswer: The best model is a CNN.\n\nAnswer: What is the best model?\n\nAnswer: What is the best model?\n\nAnswer: What is the best model?\n\nAnswer: What is the best model?\n\nAnswer: What is the best model?\n", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "English\n\nQuestion: What is the main contribution of this paper.\n\nQuestion: What is the main contribution of the paper.\n\nAnswer: The paper is a shared task.\n\nQuestion: What is the shared task.\n\nAnswer: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared task.\n\nQuestion: What is the shared", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The size of WikiLarge dataset is 296,402, and the size of both datasets are 296,402.\n\nQuestion: what are the sizes of both datasets?\n\nAnswer: The size of the simplified data is 600,0000,0000,0000,0000,00000,0000,0000000000000000000000000000000000", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "\n\nQuestion: What is the pre-training baseline.\n\nQuestion: What is the pre-training baseline?\n\nAnswer: What is the pre-training baseline.\n\nQuestion: What is the pre-training baseline.\n\nAnswer: What is the pre-training baseline.\n\nAnswer: What is the pre-training baseline.\n\nQuestion: What is the pre-training baseline.\n\nAnswer: What is the pre-training baseline.\n\nAnswer: What is the pre-training.\n\nAnswer: What is the pre-training.\n\nAnswer", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "The combination of rewards for reinforcement learning is the combination of rewards.\n\nQuestion: What is the combination of rewards for reinforcement learning.\n Question: What is the combination of rewards.\n\nQuestion: What is the combination of rewards for reinforcement.\n\nQuestion: What is the combination of the combination of the rewards.\n\nQuestion: What is the combination of the combination of the combination of the combination of the reward.\n\nQuestion: What is the combination of the combination of the reward.\n\nQuestion: What is the combination of the reward.\n\nQuestion: What is the combination of the reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "\n\nQuestion: What was the new dataset?\n\nQuestion: What was their results on the new dataset?\n\nAnswer: What were their results on the new dataset?\n\nAnswer: What were their results on the new dataset?\n\nQuestion: What was their results on the new dataset?\n\nAnswer: What were their results on the new dataset?\n\nAnswer: What was their results on the new dataset.\n\nAnswer: What was their results on the new dataset.\n\nAnswer: What was their results on the new dataset.\n\nAnswer: What was their", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "\n\nQuestion: Do the answered questions measure for the usefulness of the question.\n\nQuestion: Do the answered questions measure for the usefulness of the question.\n\nAnswer: The question asker's question is the question.\n\nQuestion: Do the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the question is the", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\n\nQuestion: What were their distribution results?\n\nQuestion: What were their distribution results?\n\nAnswer: What were their distribution results?\n\nQuestion: What were their distribution results?\n\nQuestion: What were their distribution results?\n\nQuestion: What were their distribution results?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion: What were their distribution?\n\nQuestion:", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "Since we do not have an end-to-end-to-end model for the image-to-poem.\n\nQuestion: What is the model's limitations?\n\nAnswer: The model is a CNN-RNN based image-to-based on the model.\n\nQuestion: What is the model?\n\nAnswer: The model is a CNN-based model.\n\nQuestion: What is the model?\n\nAnswer: The model is a model.\n\nQuestion: What is the model?\n\nAnswer: The model is a model.\n\nQuestion: What", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "\n\nQuestion: What is the dataset of hashtag segmentation.\n\nQuestion: How is the dataset of hashtag segmentation.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the dataset of hashtag.\n\nQuestion: How is the", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "unanswerable\n\nQuestion: Is SemCor3.0 is reflective of English language data in general?\n\nQuestion: Is SemCor3.0 is reflective of the English language data.\n\nAnswer: Is SemCor3.0 is reflective of the English language.\n\nAnswer: Is SemCor3.0 is a reflective of the English language.\n\nAnswer: Is SemCor3.0 is a reflective of the English language.\n\nAnswer: Is SemCor3.0 is a reflective of the English language.\n\nAnswer: Is Sem", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "\n\nQuestion: what is the most challenging trials in the database.\n\nAnswer: unanswerable\n\nQuestion: what is the most challenging trials are?\n\nAnswer: unanswerable\n\nQuestion: what is the most challenging trials.\n\nAnswer: unanswerable\n\nQuestion: what is the most challenging trials.\n\nAnswer: unanswerable\n\nQuestion: what is the most challenging trials.\n\nAnswer: unanswerable\n\nQuestion: what is the most challenging trials.\n\nQuestion: what is the most challenging trials.\n\nAnswer: unanswer", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "\n\nQuestion: What is the overall performance of the model is used?\n\nAnswer: The baseline model is used.\n\nQuestion: What is the best performing baseline model.\n\nQuestion: What is the best performing model is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best performing model.\n\nQuestion: What is the best", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Augmented LibriSpeech is a large-resource dataset.\n\nQuestion: How big is Augmented LibriSpeech is a large-speech.\n\nAnswer: How big is the LibriSpeech is a large-speech.\n\nAnswer: How big is the Libri-speech is a large-speech.\n\nAnswer: How big is the Libri-speech.\n\nAnswer: How big is the Libri-speech.\n\nAnswer: How big is the Libri-speech.\n\nAnswer: How big", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "Yes.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes.\n\nQuestion: Are the automatically constructed datasets are subject to quality control?\n\nQuestion: Are the automatically constructed datasets are subject to quality control.\n\nQuestion: Are the automatically constructed datasets are subject to quality control.\n\nQuestion: Are the automatically constructed datasets are subject to control.\n\nQuestion: Are the automatically constructed datasets are subject to quality control.\n\nQuestion: Are the automatically constructed datasets are subject to control.\n\nQuestion: Are the automatically constructed datasets are subject to control.\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "They use small BERT.\n\nQuestion: What is the best configuration of the IMS framework, which is a gloss-augmented memory network.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the IMS.\n\nQuestion: What is the best configuration of the", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "GloVe and Edinburgh embeddings.\n\nQuestion: what pretrained word embeddings were used?\n\nAnswer: GloVe embeddings\n\nAnswer: GloVe\n\nQuestion: what pretrained word embeddings\n\nAnswer: GloVe\n\nAnswer: GloVe\n\nAnswer: GloVe\n\nQuestion: what pretrained word embeddings\n\nAnswer: GloVe\n\nAnswer: GloVe\n\nQuestion: GloVe\n\nQuestion: GloVe\n\nQuestion: G", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: yes\n\nQuestion: Are the images from a specific domain?\n\nAnswer: yes\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the domain?\n\nAnswer: Are the domain?\n\nAnswer: Are the domain?\n\nAnswer: Are the domain of the domain.\n\nAnswer: Are the domain of the domain", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Yes.\n\nQuestion: Is Arabic is one of the 11 languages in CoVost.\n\nQuestion: Is Arabic is one of the 11.\n\nAnswer: Yes, Arabic is one of the 11.\n\nQuestion: Is Arabic is one of the 11.\n\nAnswer: Is Arabic is Arabic is one of the 11.\n\nAnswer: Is Arabic is one of the 11.\n\nQuestion: Is Arabic is one of the 11.\n\nAnswer: Is Arabic is", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "\n\nQuestion: What is the tagging scheme employed?\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:\n\nQuestion: What is the tagging scheme employed?\n\nQuestion: What is the tagging scheme?\n\nQuestion: What is the tagging scheme employed?\n\nQuestion: What is the tagging scheme?\n\nAnswer:\n\nQuestion: What is the tagging scheme?\n\nQuestion: What is the tagging scheme?\n\nQuestion: What is the tagging scheme?\n\nQuestion: What is the tagging scheme?\n\nQuestion:", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "\n\nQuestion: How do they define robustness of a model?\n\nAnswer: The robustness of a model is defined as robustness of a model.\n\nQuestion: How do they define robustness of a model.\n\nAnswer: The robustness of a model.\n\nAnswer: The robustness of a model.\n\nAnswer: The robustness of a model.\n\nAnswer: The robustness of a model.\n\nQuestion: How do the robustness of a model.\n\nAnswer: The robustness of a model.\n\nQuestion: How do the", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "They test their conflict method on two tasks.\n\nQuestion: What is the difference between attention and conflict.\n\nAnswer: They test their conflict method on the task of the task.\n\nQuestion: What is the task of the task.\n\nAnswer: The task of the task.\n\nAnswer: The task is the task.\n\nQuestion: What is the task.\n\nAnswer: The task.\n\nAnswer: the task.\n\nAnswer: task.\n\nAnswer: The task.\n\nAnswer: task.\n\nAnswer: task.\n", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "\n\nThe proposed method's improvements of F1 for NER task for English and Chinese datasets.\n\nQuestion: What are the results of the method's improvements of F1 for NER.\n\nAnswer: What are the improvements of F1 for NER.\n\nAnswer: What are the improvements of the F1 for NER.\n\nAnswer: What are the improvements of the NER.\n\nAnswer: What are the improvements of the F1 for NER.\n\nAnswer: What are the improvements of the F1 for NER.\n\nAnswer:", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "\n\nThe methods are:\n\n1. Part-of-speech information.\n\nQuestion: What is the difference between the two types of the baby is marked as Asian.\n\nAnswer: The Flickr30K dataset is a collection of the baby.\n\nQuestion: What is the Flickr30K dataset.\n\nAnswer: The Flickr30K dataset is a collection of the baby.\n\nAnswer: The Flickr30K dataset is a collection of the baby.\n\nAnswer: The Flickr30K dataset is a", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "\n\nQuestion: What is the best numbers we report for SATA-LSTM, and the results of the model.\n\nQuestion: What is the best model for the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion: What is the SST-2.\n\nQuestion", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The baseline models are encoder-decoder.\n\nQuestion: What are the baseline models?\n\nAnswer: The baseline models are the baseline.\n\nQuestion: What are the baseline models?\n\nAnswer: The baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the baseline models.\n\nQuestion: What are the", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The authors experimented with Sumy.\n\nQuestion: What is the ILP-based summarization algorithm.\n\nQuestion: What is the ILP-based summarization algorithm.\n\nAnswer: What is the ILP-based summarization algorithm.\n\nAnswer: What is the ILP-based summarization algorithm.\n\nAnswer: What is the ILP-based summarization algorithm.\n\nAnswer: The ILP-based summarization algorithm.\n\nAnswer: The ILP-based summarization algorithm.\n\nAnswer: The ILP-based summarization algorithm.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "English\n\nQuestion: What is the Winograd schema?\n\nAnswer: The Winograd schema is a pair of sentences, or short texts, or of short texts, or short texts, or short texts, or short texts, or short texts.\n\nQuestion: What is the Winograder, or short texts, or short texts, or short texts.\n bekan, or short texts, or short texts, or short texts.\n\nQuestion: What is the Winograder, or short texts, or short texts, or short texts, or short texts, or short texts, or short texts", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "No, they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nAnswer: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nAnswer: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nQuestion: Do they report results on English data.\n\nAnswer: Do they report results on English data.\n\nQuestion: Do they report results", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "\n\nQuestion: What is the best model on the development set.\n\nAnswer:\n\nQuestion: What is the best model on development data.\n\nAnswer:\n\nQuestion: What is the best model.\n\nAnswer:\n\nQuestion: What is the best model:\n\nQuestion: What is the best model:\n\nAnswer:\n\nQuestion: What is the best model:\n\nAnswer:\n\nQuestion: What is the best model:\n\nAnswer:\n\nQuestion: What is the best model:\n\nAnswer:\n\nQuestion: What is", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, Hindi, Malayal, Kannada, and English.\n\nQuestion: Which 7 Indian languages do they experiment with?\n\nAnswer: Which 7 Indian languages they experiment with the previous state of the art language identification approach.\n\nQuestion: Which 7 Indian languages.\n\nAnswer: Which 7 Indian languages.\n\nQuestion: Which 7 Indian languages.\n\nQuestion: Which 7 Indian languages.\n\nQuestion: Which 7 Indian languages.\n\nQuestion: Which 7 Indian", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "\n\nQuestion: What was the previous state of the art for this task?\n\nQuestion: What was the previous state of the art for this task?\n\nAnswer: What was the state of the art of this task?\n\nQuestion: What was the state of the art of this task.\n\nQuestion: What was the state of the art of this task.\n\nQuestion: What was the state of the art of this task.\n\nQuestion: What was the state of the art of this task.\n\nQuestion: What was the state of the art of this task.\n\nQuestion: What", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "DTA18\n\nQuestion: What is the corpus used for the task?\n\nAnswer: The corpus used in the task is the corpus used for the task is the corpus used for the task.\n\nQuestion: What is the corpus used in the task.\n\nAnswer: The corpus used for the task.\n\nQuestion: What is the corpus used in the task.\n\nAnswer: What is the corpus used in the task.\n\nAnswer: What is the corpus used in the task.\n\nAnswer: What is the corpus", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The authors present evidence that the model can capture some biases in data annotation.\n\nQuestion: What is the best fine-tuning strategy.\n\nAnswer: The authors present that the model can capture some biases in data annotation.\n\nQuestion: What evidence the model can capture some biases in data annotation.\n\nQuestion: What is the model can capture some biases in the data.\n\nQuestion: What is the model can capture some biases in the data.\n\nQuestion: What is the model can capture some biases in the data.\n\nQuestion: What is the", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "\n\nQuestion: What is the model performance on target language reading comprehension?\n\nQuestion: What is the model performance on target language comprehension.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is the model performance on target language.\n\nQuestion: What is", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "\n\nQuestion: How much improvement is gained from Adversarial Reward Augment Maximum Likelihood\n\nAnswer: How much improvement is gained from ARAML\n\nQuestion: How much improvement is gained from ARAML\n\nAnswer: How much improvement from ARAML\n\nAnswer: How much improvement is gained from ARAML\n\nQuestion: How much improvement is gained from ARAML\n\nAnswer: How much improvement is gained from ARAML\n\nAnswer: How much improvement is gained from ARAML\n\nAnswer: How much improvement is gained from", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The datasets used are:\n\nQuestion: What is the dataset used?\n\nAnswer: The dataset used in the article.\n\nQuestion: What is the dataset used in the article.\n\nAnswer: The dataset is used in the article.\n\nAnswer: The dataset is used in the article.\n\nAnswer: The dataset is used in the article.\n\nAnswer: The dataset is used in the article.\n\nAnswer: The dataset is used in the article.\n\nAnswer: The dataset is used in the article.\n\nThe dataset is used in the article.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "Yes, we tested the word count baseline.\n\nQuestion: What is the best performing baseline, Bert + Unanswerable.\n\nAnswer: The best performing baseline is the best performing baseline.\n\nQuestion: What is the best baseline is the best baseline.\n\nAnswer: The best baseline is the best answer.\n\nAnswer: The best answer is the best answer is the best answer.\n\nQuestion: The best answer is the best answer is the best answer is the best answer is the best answer is the best answer is the best answer is the best answer is the best answer is the best answer", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "\n\nThe proposed method improves F1 score by +0.58 for paraphrase.\n\nQuestion: What are the proposed method improvements of F1 for paraphrase.\n\nQuestion: What are the method improvements of F1 for paraphrase.\n\nQuestion: What are the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of the method of", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The language model architectures are used.\n\nQuestion: What is the language model architectures.\n\nQuestion: What language model architectures.\n\nQuestion: What language model architectures.\n\nAnswer: What language model architectures.\n\nQuestion: What language model architectures.\n\nAnswer: What language model architectures.\n\nQuestion: What language model architectures.\n\nAnswer: What language model architectures.\n\nQuestion: What language model architectures.\n\nQuestion: What language model architectures.\n\nQuestion: What language model architectures.\n\nQuestion: What", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The subjects were presented with the KARA ONE dataset.\n\nQuestion: What is the best architecture for the CNN-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-LSTM-L", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Word subspace can represent the content of texts.\n\nQuestion: What is the word subspace representation.\n\nAnswer: The word subspace representation of the word2vec.\n\nAnswer: What is the word subspace representation.\n\nAnswer: What is the word2vec.\n\nAnswer: What is the word2vec.\n\nAnswer: What is the word2vec.\n\nAnswer: What is the word2vec.\n\nAnswer: What is the word2vec.\n\nQuestion: What is the word2vec.\n\nAnswer: What", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "\n\nQuestion: What is the most frequently studied machine learning models.\n\nAnswer:\n\nQuestion: What is the most accurate in classifying abusive language.\n\nAnswer:\n\nQuestion: What is the most frequently studied on the most significant character-level features.\n\nAnswer:\n\nQuestion: What is the most significant character-level features.\n\nAnswer:\n\nQuestion: What is the most significant character-level features.\n\nAnswer:\n\nQuestion: What is the most significant character-level features.\n\nAnswer:\n\nQuestion: What is the", "answers": ["Nave Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Nave Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "\n\nQuestion: How is non-standard pronunciation is identified?\n\nAnswer:\n\nQuestion: How is non-standard pronunciation?\n\nQuestion: How is non-standard pronunciation?\n\nQuestion: How is non-standard pronunciation?\n\nQuestion: How is non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation.\n\nQuestion:", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "\n\nQuestion: How are weights dynamically adjusted.\n\nAnswer: The weights are dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights dynamically adjusted.\n\nAnswer: How are weights are", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "\n\nQuestion: What dataset did they use.\n\nQuestion: What dataset did they use.\n\nQuestion: What dataset did they use.\n\nQuestion: What dataset did they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset they use.\n\nQuestion: What dataset.\n\nQuestion: What dataset.\n\nQuestion:", "answers": [" high-quality datasets  from SemEval-2016 Sentiment Analysis in Twitter task", " SemEval-2016 Sentiment Analysis in Twitter"], "all_classes": null, "length": 2738}
{"pred": "An individual model consists of a monolingual model.\n\nQuestion: What is the main results of the model.\n\nQuestion: What is the monolingual model.\n\nQuestion: What is the monoling model.\n\nQuestion: What is the monol model.\n\nQuestion: What is the monol model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\n", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Yes\n\nQuestion: Is the data is de-identified?\n\nQuestion: Is the data de-identified?\n\nAnswer: Is the data de-identified?\n\nQuestion: Is the data de-identified?\n\nQuestion: Is the data de-identified?\n\nAnswer: Is the data de-identified?\n\nQuestion: Is the data de-identified?\n\nAnswer: Is the data de-identified?\n\nQuestion: Is the data de-identified?\n\nQuestion: Is the data de-identified?\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "A semicharacter architecture is a type of architecture.\n\nQuestion: What is a semicharacter architecture.\n\nQuestion: What is a semicharacter architecture.\n\nQuestion: What is a semicharacter architecture.\n\nQuestion: What is a semicharacter architecture.\n\nQuestion: What is a semicharacter architecture.\n\nQuestion: What is a semicharacter.\n\nQuestion: What is a semicharacter.\n\nQuestion: What is a semicharacter.\n\nQuestion: What is a semicharacter.\n\nQuestion", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: What was their performance on emotion detection?\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: What was their performance on emotion detection?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their performance?\n\nAnswer: What was their", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The NCEL approach is effective overall.\n\nQuestion: What is the best performance of NCEL.\n\nAnswer: The NCEL is a neural model for collective entity linking.\n\nQuestion: How does the NCEL approach?\n\nAnswer: The NCEL is a neural network that is a neural network.\n\nQuestion: What is the NCEL is a neural network.\n\nQuestion: What is the NCEL is a neural network.\n\nQuestion: What is the NCEL is a neural network.\n\nQuestion: What is the NCEL", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "\n\nQuestion: What is the objective function?\n\nAnswer:\n\nQuestion: What is the objective function?\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nQuestion:\n\nAnswer:\n\nAnswer:\n\nQuestion:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The baseline was the system that was used in the article.\n\nQuestion: What was the baseline used?\n\nAnswer: The baseline was the system that was used in the article.\n\nAnswer: The baseline was the system that was used in the article.\n\nAnswer: The baseline was the system that was used in the article.\n\nAnswer: The baseline was the system that was used in the article.\n\nAnswer: The baseline was the system that was used in the article that was used in the article that was used in the system that was used in the article that was used in", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "\n\nQuestion: What is the average performance of SBERT is evaluated on the STS benchmark.\n\nQuestion: What is the performance of the sentence embeddings methods are evaluated.\n\nQuestion: What is the performance of the sentence embeddings.\n\nQuestion: What is the sentence embeddings are evaluated on the sentence embeddings.\n\nQuestion: What is the sentence embeddings are evaluated.\n\nQuestion: What is the sentence embeddings are evaluated.\n\nQuestion: What is the sentence embeddings are evaluated.\n\nQuestion: What is the sentence embeddings are evaluated.\n\nQuestion: What is", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "\n\nQuestion: Why masking words in the decoder is helpful.\n\nQuestion: Why masking words in the decoder is helpful.\n\nAnswer:\n\nQuestion: Why masking words in the decoder is helpful.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAn", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The training data was translated using the machine translation platform Apertium.\n\nQuestion: What is the best model for the emotion intensity tasks.\n\nAnswer: The training data was translated using the machine translation.\n\nQuestion: How was the training data was translated.\n\nAnswer: The training data was translated.\n\nQuestion: How was the training data was translated.\n\nAnswer: The training data was translated.\n\nAnswer: The training data was translated.\n\nAnswer: The training data was translated.\n\nAnswer: The training data was translated.\n\nAnswer:", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "TF-IDF features.\n\nQuestion: What is the primary objective of the article.\n\nAnswer: What features are used?\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.\n\nAnswer: What is the primary diagnosis.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "\n\nQuestion: What is the BioBioBERT model?\n\nQuestion: What is the BioBERT model?\n\nAnswer:\n\nQuestion: What is the BioBERT model?\n\nQuestion: What is the BioBERT model?\n\nQuestion: What is the BioBERT model?\n\nQuestion: What is the BioBERT model?\n\nQuestion: What is the BioBERT model?\n\nQuestion: What is the BioBERT?\n\nQuestion: What is the BioBERT?\n\nQuestion: What is the BioBERT?\n\nQuestion:", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "\n\nQuestion: What is the core component for KBQA?\n\nAnswer:\n\nQuestion: What is the core component for KBQA?\n\nAnswer: What is the core component for KBQA.\n\nQuestion: What is the core component for KBQA.\n\nQuestion: What is the core component for KBQA.\n\nQuestion: What is the core component for KBQA.\n\nQuestion: What is the core component for KBQA.\n\nQuestion: What is the core component for KBQA.\n\n", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "\n\nQuestion: How is the political bias of different sources included in the model.\n\nQuestion: How is the political bias of different sources included in the model.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias of different sources.\n\nAnswer: How is the political bias", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "\n\nQuestion: What is the best result on the test data.\n\nAnswer: What is the best result on the development set.\n\nQuestion: What is the best model did not use the development set.\n\nAnswer: What is the best model.\n\nAnswer: What is the best model did not use the best model.\n\nAnswer: What is the best model.\n\nQuestion: What is the best model.\n\nAnswer: What is the best model.\n\nQuestion: What is the best model.\n\nAnswer: What is the best model.\n\n", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "\n\nQuestion: What baselines do they compare with?\n\nAnswer: What baselines they compare with.\n\nQuestion: What baselines they compare with?\n\nAnswer: What baselines they compare with?\n\nQuestion: What baselines they compare them.\n\nAnswer: What baselines them compare them.\n\nQuestion: What baselines they compare them.\n\nAnswer: What baselines them.\n\nAnswer: What baselines them.\n\nQuestion: What baselines them.\n\nQuestion: What baselines them.\n\nQuestion: What baselines them.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nAnswer:\n\nQuestion: What is the best model?\n\nQuestion: What is the best model?\n\nAn", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "3\n\nQuestion: What is the main contribution of the UTCNN model?\n\nAnswer: The FBFans dataset is a single-topic dataset.\n\nQuestion: What is the main contribution of the UTCNN model?\n\nAnswer: The main contribution of the UTCNN model.\n\nQuestion: What is the UTCNN model?\n\nAnswer: The main contribution of the UTCNN model.\n\nQuestion: What is the UTCNN model.\n\nAnswer: The main contribution of the model.\n\nAnswer: The main contribution of the model.\n\nAnswer:", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The ancient Chinese dataset comes from the internet.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nAnswer: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nAnswer: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the ancient Chinese dataset.\n\nQuestion: What is the", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "\n\nQuestion: what is the compound PCFG?\n\nAnswer:\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\nQuestion: what is the compound PCFG?\n\n", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "\n\nQuestion: Which component is the least impactful.\n\nQuestion: Which component is the least impactful.\n\nQuestion: Which component is the least impactful.\n\nAnswer:\n\nQuestion: Which component is the least impactful.\n\nQuestion: Which component is the least impactful.\n\nAnswer: Which component is the least impact.\n\nQuestion: Which component is the least impactful.\n\nQuestion: Which component is the least impact.\n\nQuestion: Which component is the least impact.\n\nQuestion: Which component is the least impact.\n\nQuestion: Which component", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: what dataset is used in this paper?\n\nAnswer: what dataset is used in this paper.\n\nQuestion: what is the dataset used in this paper.\n\nAnswer: what is the paper is the dataset.\n\nAnswer: what is the paper is the paper.\n\nAnswer: what is the paper is the dataset.\n\nAnswer: what is the paper is the dataset.\n\nAnswer: what is the paper is the dataset.\n\nAnswer: what is the paper is the", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation.\n\nQuestion: What is the knowledge base completion problem.\n\nAnswer: The knowledge base is a new approach to the knowledge base.\n\nQuestion: What is the knowledge base.\n\nAnswer: The knowledge base is a new approach to the knowledge base.\n\nQuestion: What is the knowledge base is a new to the knowledge base.\n\nAnswer: The knowledge base is a new to the knowledge base.\n\nQuestion: What is the knowledge base.\n\nQuestion: What is the knowledge", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Unigram features, unigram features, and sarcasm features.\n\nQuestion: What is the cognitive features.\n\nAnswer: The cognitive features are the cognitive features.\n\nQuestion: What is the cognitive features.\n\nAnswer: The cognitive features.\n\nAnswer: What is the cognitive features.\n\nAnswer: What is the cognitive features.\n\nAnswer: What is the cognitive features.\n\nAnswer: What is the cognitive features.\n\nAnswer: The cognitive features.\n\nQuestion: What is the cognitive features.\n\nAnswer: The cognitive", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "No, they use Lucene to create a sample of a QA Wikipedia dataset.\n\nQuestion: What is the best model for answer selection.\n\nAnswer:\n\nQuestion: What is the best model for the QA.\n\nAnswer: What is the best model for QA.\n\nAnswer: What is the best model for QA.\n\nAnswer: What is the best model for QA.\n\nAnswer: What is the best model for QA.\n\nAnswer: What is the best model for QA.\n\nAnswer: What is the best", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "The proposed model outperformed the baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines.\n\nQuestion: How big is the difference in performance between the model and baselines.\n\nQuestion: How big is the difference in performance between the model and baselines.\n\nQuestion: How big is the difference in performance between the model.\n\nQuestion: How big is the difference in performance between the", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "\n\nQuestion: How does Gaussian-masked directional attention works?\n\nAnswer: How does Gaussian-masked attention?\n\nAnswer: How does Gaussian-masked attention?\n\nAnswer: How does the attention of the attention?\n\nQuestion: How does the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the attention of the of the attention of the attention of the attention of", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "\n\nQuestion: What is the main contribution of the article?\n\nAnswer: What is the main contribution of the article?\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAnswer: What is the main contribution of the article.\n\nAn", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Facebook\n\nQuestion: What is the causal explanation?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the two types of social media.\n\nQuestion: What is the difference between the causal explanation.\n\nQuestion: What is the difference between the causal explanation.\n\nQuestion: What is the difference between the causal explanation.\n\nQuestion: What is the difference between the causal explanation.\n\nQuestion: What is the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "\n\nTheir system ranked second, second (EI-Reg, EI-Oc, second (EI-Oc, their system was the best models were the scores of their system.\n\nQuestion: What were the scores of their system.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nQuestion: What were the scores of their system.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\n", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The hyperparameters varied in the experiments on the four tasks.\n\nQuestion: What is the best results are generally obtained in the task of the task.\n\nAnswer: The best results are generally obtained in the task of the task.\n\nQuestion: Which hyperparameters were varied in the experiments on the four tasks.\n\nAnswer: The best results are generally obtained in the task of the task.\n\nAnswer: The best results are the task of the task.\n\nQuestion: The best results are the task of the task.\n\nAnswer: The best results are the task of the task.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The corpus is 53 documents.\n\nQuestion: How many entities are annotated in the corpus.\n Question: How large is the corpus.\n\nAnswer: The corpus is 10000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "\n\nQuestion: What is the model is their model compared to previous methods?\n\nQuestion: What previous methods is their model compared to previous methods is their model compared to previous methods.\n\nQuestion: What is their model compared to a previous method.\n\nQuestion: What is their model compared to previous model compared to previous model compared to previous model compared to previous model compared to previous model compared to previous.\n\nQuestion: What is their model compared to previous model compared to previous model compared to previous model compared to previous.\n\nQuestion: What is their model compared to previous to previous to previous.\n\nQuestion:", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "Sentiment analysis\n\nQuestion: What is the article is about.\n\nQuestion: What NLP tasks do they consider?\n\nQuestion: What NLP tasks do they consider.\n\nQuestion: What NLP tasks do they consider.\n\nQuestion: What NLP tasks do they consider.\n\nQuestion: What NLP tasks do they consider.\n\nQuestion: What NLP tasks they consider.\n\nQuestion: What NLP tasks they consider.\n\nQuestion: What NLP tasks they consider.\n\nQuestion: What NLP tasks they consider.\n\nQuestion: What NLP tasks they", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The training sets of these versions of ELMo are larger.\n\nQuestion: What is the difference between the training sets of these versions of ELMo.\n\nQuestion: What is the difference between the training sets of these versions of ELMo.\n\nQuestion: What is the difference between the training sets of these versions of the ELMo.\n\nQuestion: What is the training sets of the training sets of the training sets of the training sets of the training sets of the training sets of the training sets of the training sets of the training sets of the training sets of the training sets.\n\nQuestion: What is the training", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "Yes.\n\nQuestion: Does their NER model learn NER from both text and images.\n\nQuestion: Does their NER model learn NER from both text and images.\n\nQuestion: Does their NER model learn NER from text and image.\n\nQuestion: Does their NER model learn NER.\n\nQuestion: Does their NER model learn NER.\n\nQuestion: Does their NER model learn NER.\n\nQuestion: Does their NER model learn NER.\n\nQuestion: Does their NER model learn NER.\n\nQuestion: Does their NER", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The authors propose a new framework to learn the class discriminative characteristics, even from low data resources.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the article.\n\nQuestion: What is the main contribution of the", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "\n\nQuestion: What is the size of the dataset?\n\nAnswerable\n\nQuestion: What is the size of the dataset?\n\nAnswer: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n\nAnswer: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n\nQuestion: What is the size of the dataset?\n", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "\n\nQuestion: Do they evaluate on English datasets?\n\nAnswer: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they evaluate on English datasets.\n\nQuestion: Do they", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "The authors provide a survey among engineers and identify three types of engineers.\n\nQuestion: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques.\n\nQuestion: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks.\n\nAnswer: The authors evidence that many engineers find it a big overhead to choose from multiple frameworks.\n\nQuestion: How do the authors evidence the claim that many engineers find it a big overhead to choose the frameworks.\n\nAnswer: The authors of the", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "\n\nQuestion: What is the best model?\n\nQuestion: What is the model?\n\nQuestion: What is the model?\n\nQuestion: What is the model?\n\nQuestion: What is the model?\n\nQuestion: What is the model?\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n\nQuestion: What is the model.\n", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "SimpleQuestions and WebQSP\n\nQuestion: What is the state of the art?\n\nAnswer: On which benchmarks they achieve the state of the art?\n\nAnswer: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n\nQuestion: On which benchmarks?\n", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "Pointer-Gen, Pointer-Gen+RL-ROU20, Pointer-RL-SEN, Pointer-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-S, Pointer, Pointer-ARL-SEN, Pointer-ARL-SEN, Pointer-ARL-SEN, Pointer-AR", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "\n\nQuestion: What are the results from these proposed strategies?\n\nAnswer:\n\nQuestion: What are the results from these proposed strategies.\n\nAnswer: What are the results from these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: What are these results.\n\nAnswer: are these results.\n\nAnswer:", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "\n\nQuestion: what is the best performing model?\n\nQuestion: which languages are explored?\n\nQuestion: which languages are explored?\n\nAnswer: which languages are explored.\n\nQuestion: which languages are explored.\n\nQuestion: which languages are explored.\n\nQuestion: which languages are explored.\n\nQuestion: which languages are explored.\n\nQuestion: which languages are explored.\n\nQuestion: which languages.\n\nQuestion: which languages.\n\nQuestion: which languages.\n\nQuestion: which languages.\n\nQuestion: which languages.\n\nQuestion: which languages.\n\n", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "\n\nQuestion: what is the hybrid data?\n\nAnswer:\n\nQuestion: what is the hybrid data?\n\nAnswer:\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer:\n\nAnswer:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "The dataset is annotated as no evidence of depression.\n\nQuestion: What is the dataset annotated?\n\nAnswer: The dataset is annotated.\n\nQuestion: What is the dataset annotated.\n\nAnswer: The dataset is the annotated.\n\nQuestion: How is the dataset annotated.\n\nAnswer: The dataset is the notated.\n\nAnswer: The dataset is the notated.\n\nAnswer: The dataset is the notated.\n\nAnswer: The dataset is the notated.\n\nAnswer: The dataset", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The baseline for the SLC task is a very simple logistic regression.\n\nQuestion: What was the baseline for the SLC task.\n\nQuestion: What was the baseline for the SLC task?\n\nQuestion: What was the baseline for the SLC task?\n\nQuestion: What was the baseline for the Sentence level classification task.\n\nAnswer: The baseline for the baseline for the Sentence level.\n\nQuestion: What was the baseline for the baseline for the Sentence level.\n\nQuestion: What was the baseline for the Sentence level.\n\nQuestion", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "English\n\nQuestion: What is the target of the offensive language identification experiment.\n\nQuestion: In what language are the tweets?\n\nAnswer: English\n\nQuestion: In what language are the tweets\n\nAnswer: In what language are the tweets\n\nAnswer: In what language are the tweets\n\nAnswer: In what language are the tweets\n\nAnswer: In what language are the tweets\n\nAnswer: In what language are the tweets\n\nAnswer: In the tweets\n\nAnswer: In the tweets\n\nQuestion: In", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the BERT model?\n\nAnswer: BERT is a pre-trained model.\n\nQuestion: What is the BERT model.\n\nAnswer: BERT is a pre-trained model.\n\nQuestion: What is the BERT model.\n\nQuestion: What is the BERT model.\n\nQuestion: What is the BERT model.\n\nQuestion: What is the BERT model.\n\nQuestion: What is the BERT model.\n\nQuestion: What is the BERT model.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Galatasaray and Fenerbah.\n\nQuestion: Which sports clubs are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets are the targets.\n\nAnswer: The targets", "answers": ["Galatasaray, Fenerbahe", "Galatasaray , Fenerbahe "], "all_classes": null, "length": 2234}
{"pred": "\n\nQuestion: What are the network's baseline features?\n\nAnswer: unanswerable\n\nQuestion: What are the network's baseline features?\n\nAnswer:\n\nQuestion: What are the network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network's network", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "Yes.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-style questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-style questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-style questions.\n\nQuestion: Is it possible to convert a naturally-style questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-style questions.\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "\n\nQuestion: How many entities are there in the dataset?\n\nAnswerable\n\nQuestion: How many entities are there in the dataset.\n\nQuestion: How many entities are there in the dataset.\n\nQuestion: How many entities are there in the dataset.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n\nQuestion: How many entities are there.\n", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "\n\nQuestion: What was their highest MRR score?\n\nQuestion: What was their highest MRR score?\n\nAnswer:\n\nQuestion: What was their highest MRR score?\n\nQuestion: What was their highest MRR score?\n\nAnswer:\n\nAnswer:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nAnswer:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:\n\nQuestion:", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
